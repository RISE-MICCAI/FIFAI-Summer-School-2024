{"metadata":{"colab":{"provenance":[],"include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":83099,"databundleVersionId":9169362,"sourceType":"competition"},{"sourceId":9045063,"sourceType":"datasetVersion","datasetId":5453198}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/prabinbohara10/Federated-Learning/blob/main/notebooks/FIFAI_Kaggle_Competition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"Changes made:\n* Data Augumentation to handle class imabalnce problem. \n    - Used SMOTE technique \n    - Tried Random and Weighted Sampler\n    - tried weighted cross entropy loss.\n* Total round of 2 and 5\n    - Each local epoch of 50, 100 and 200","metadata":{}},{"cell_type":"markdown","source":"# 🌟 Welcome to the FIFAI Summer Kaggle Competition! 🌟\n\nThis Jupyter notebook is your main point of entry for the FIFAI Summer Kaggle Competition 🌞. Here, you'll have the chance to apply the skills you've learned over the past few days in a practical, competitive environment. This notebook contains everything you need to get started:\n\n- 📥 **Loading the Data**: Step-by-step instructions to load and preprocess the dataset.\n- 🏢 **Setting Up the Federated Learning Environment**: Guidance on how to set up and run federated learning experiments.\n- 📊 **Evaluation Metrics**: Tools to evaluate your models using various metrics.\n- 🧠 **Simple Models**: Starter code for basic models to kickstart your experimentation.\n- 📝 **Submission Files**: Code to help you generate and submit your results to Kaggle.\n\nDive in, explore, and have fun! We can't wait to see the innovative solutions you'll come up with. Good luck, and may the best model win! 🚀\n","metadata":{"id":"TgeIcJO_6cx5"}},{"cell_type":"markdown","source":"# Setup","metadata":{"id":"YEg6aM9V39Kw"}},{"cell_type":"markdown","source":"The following libraries are required to run this notebook. If you are running this on Colab it should be all smooth sailing. If you are running it locally please make sure you have all of these installed (take a look at the readme file at the [repo](https://github.com/RISE-MICCAI/FIFAI-Summer-School-2024/tree/main/Day%203%20-%20FIFAI%20Kaggle%20competition) for instructions on how to do this with conda).","metadata":{"id":"AuVNaNZUMPaV"}},{"cell_type":"code","source":"import os\nimport random\nimport zipfile\nfrom collections import defaultdict\nimport copy\n\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\nimport pandas as pd\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns","metadata":{"id":"WKanyuPFDjlt","execution":{"iopub.status.busy":"2024-07-29T16:23:42.449413Z","iopub.execute_input":"2024-07-29T16:23:42.449771Z","iopub.status.idle":"2024-07-29T16:23:48.170868Z","shell.execute_reply.started":"2024-07-29T16:23:42.449742Z","shell.execute_reply":"2024-07-29T16:23:48.170057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.is_available()","metadata":{"id":"icJUJZgwAKml","outputId":"e7dbefa8-a63d-4daa-c852-739b1ca36d5c","execution":{"iopub.status.busy":"2024-07-29T16:23:51.555071Z","iopub.execute_input":"2024-07-29T16:23:51.555561Z","iopub.status.idle":"2024-07-29T16:23:51.594519Z","shell.execute_reply.started":"2024-07-29T16:23:51.555531Z","shell.execute_reply":"2024-07-29T16:23:51.593517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install mlflow>=2.8.1\nimport mlflow\nmlflow.set_experiment(\"FIFAI Kaagle Competition\")","metadata":{"id":"E3bUGC25mhg8","outputId":"687e1b89-813e-4c70-d531-a2ad40667578"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloading","metadata":{"id":"MxhO4N4MFF7q"}},{"cell_type":"markdown","source":"First thing we need to do is load in the data. We will be looking at the ChestMNIST dataset (a multi-label dataset from [MedMNIST](https://github.com/MedMNIST/MedMNIST)). This dataset looks at images of lungs that are healthy vs unhealthy for various diseases. We have taken a subset of this data and converted into a binary classification dataset. It consists of a training set with both images and labels and a test set with just images which you will use to make predictions and submit to Kaggle. Be sure to **download all** the files on the Kaggle site before proceeding with the next steps.\n\n\n\n","metadata":{"id":"jXWuhLGWxS_h"}},{"cell_type":"markdown","source":"### Using Colab (skip if running locally)\nThe below is for use in a Colab notebook. Make a new folder called 'FIFAI_Kaggle' in your own Google Drive account. Simply upload everything into this 'FIFAI_Kaggle' folder. **NB** compress the images folder into a .zip file before uploading it otherwise it will take very long to upload. So upload 'images.zip', 'train.csv', 'test.csv', 'train_dataset.pkl' and 'test_dataset.pkl'. After that you can run the blocks of code below. First let's create a link to this new folder you created for ease of use.","metadata":{"id":"I5sLkv9UinmV"}},{"cell_type":"code","source":"# mount drive and create symlink\nfrom google.colab import drive\ndrive.mount('/content/drive')\ndata_folder_name = 'FIFAI_Kaggle'\n# make sure to check if this is the correct directory for your Google account some people do not have a space in 'My Drive'\n!ln -s \"/content/drive/My Drive/FIFAI_Kaggle\" \"/content/FIFAI_Kaggle\" # NB when changing this","metadata":{"id":"aeVtsaROccV2","outputId":"f1ba63f9-861d-47dc-ec9d-25ce2b4722c6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unzip the images zip file. This should take about 2-3 mins. **Do not** run it again if you have already unzipped the folder. No need to run this block if you are running this locally.","metadata":{"id":"k97KYnBJngLz"}},{"cell_type":"code","source":"# only run this once to unzip your files\nimport zipfile\nimport os\n\ndef unzip_folder(folder_path, extract_to=None):\n    \"\"\"\n    Unzip the given folder.\n\n    Parameters:\n    folder_path (str): Path to the zipped folder.\n    extract_to (str): Directory to extract files to. Defaults to the same directory as the zipped folder.\n\n    Returns:\n    None\n    \"\"\"\n    if extract_to is None:\n        extract_to = os.path.dirname(folder_path)\n\n    with zipfile.ZipFile(folder_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(f'Extracted all files to {extract_to}')\n\n# Example usage:\ncwd = os.getcwd()\nunzip_folder(os.path.join(cwd,data_folder_name,'images.zip'), extract_to = os.path.join(cwd,data_folder_name))\n","metadata":{"id":"Sm6P_2x-l26R","outputId":"63f4e9e0-64f6-4ea1-c3fc-925d0c2a70c1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Only for Kaggle and dataset\ndata_folder_name = \"/kaggle/input/fifai-dataset\"\n\ntrain_img_dir = os.path.join(data_folder_name, \"images/images\", \"train\")\ntest_img_dir = os.path.join(data_folder_name, \"images/images\", \"test\")\ntrain_csv_path = os.path.join(data_folder_name,\"train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:23:58.326025Z","iopub.execute_input":"2024-07-29T16:23:58.326792Z","iopub.status.idle":"2024-07-29T16:23:58.332421Z","shell.execute_reply.started":"2024-07-29T16:23:58.326754Z","shell.execute_reply":"2024-07-29T16:23:58.331251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## do not run for for Kaggle \n# data_folder_name = '/path_to_kaggle_files_on_local_dir' # uncomment and change this to the correct directory if running locally\n\n# if you are running this locally make sure you change these directories appropriately\ntrain_img_dir = os.path.join(data_folder_name, \"images\", \"train\")\ntest_img_dir = os.path.join(data_folder_name, \"images\", \"test\")\ntrain_csv_path = os.path.join(data_folder_name,\"train.csv\")","metadata":{"id":"hgs5RePXcpDG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we define appropriate classes and functions to help with dataloading when training our models.","metadata":{"id":"WaKKOs00n3Je"}},{"cell_type":"code","source":"# Create a PyTorch Dataset\nclass ChestMNISTDataset(Dataset):\n    \"\"\"\n    This class is used to turn our dataset into a PyTorch-supported dataset.\n\n    Parameters:\n    - images: The images array from the MedMNIST dataset.\n    - labels: The corresponding labels array for each image.\n    - transform: Optional transformation to be applied to the images.\n    \"\"\"\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = torch.tensor(self.labels[idx])\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\n  # build dataset function which takes in Dataset class, img_directory and labels to generate the appropriate Dataset\ndef build_train_dataset(img_dir, img2label, transform=None):\n    # run through img directory store images as np.array in images array. Simultaneously store labels in numpy array, using img2label. Then convert labels to tensor (or you can do this in the Dataset class).\n    print('---------- Building Training Dataset ----------')\n    # print('...')\n    images = []\n    labels = []\n    total_images = len(img2label)\n    step = total_images // 10  # 10% step\n\n    for i, img_fname in enumerate(img2label):\n        img_path = os.path.join(img_dir, img_fname)\n        img = np.array(Image.open(img_path))\n        label = img2label[img_fname]\n        images.append(img)\n        labels.append(label)\n\n        # Print progress\n        if (i + 1) % step == 0:\n            progress = (i + 1) // step * 10\n            print(f'{\"-\" * (progress // 10)} {progress}% complete')\n\n    print('---------- Finished Training Dataset ----------')\n    print()\n    return ChestMNISTDataset(images, labels, transform=transform)\n","metadata":{"id":"18GCMUH6wDY1","execution":{"iopub.status.busy":"2024-07-29T16:24:05.700545Z","iopub.execute_input":"2024-07-29T16:24:05.700909Z","iopub.status.idle":"2024-07-29T16:24:05.711074Z","shell.execute_reply.started":"2024-07-29T16:24:05.700882Z","shell.execute_reply":"2024-07-29T16:24:05.710184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path)\ntrain_df","metadata":{"id":"d-WGmlBQYTH3","outputId":"170b0fef-fc0a-4538-df4e-f93197097cf8","execution":{"iopub.status.busy":"2024-07-29T16:24:21.374789Z","iopub.execute_input":"2024-07-29T16:24:21.375131Z","iopub.status.idle":"2024-07-29T16:24:21.422098Z","shell.execute_reply.started":"2024-07-29T16:24:21.375104Z","shell.execute_reply":"2024-07-29T16:24:21.421249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path) # get the csv containing the labels for the training data\nimg2label = {} # build a dictionary to help us associate each image with the appropriate label\nfor i in range(len(train_df['ID'])):\n  img2label[train_df['ID'][i]] = train_df['label'][i]\n\n#img2label","metadata":{"id":"a_4n9ziH22CN","execution":{"iopub.status.busy":"2024-07-28T04:23:26.012143Z","iopub.execute_input":"2024-07-28T04:23:26.013045Z","iopub.status.idle":"2024-07-28T04:23:26.125737Z","shell.execute_reply.started":"2024-07-28T04:23:26.012998Z","shell.execute_reply":"2024-07-28T04:23:26.124648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set a seed DO NOT change this\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# preprocessing (transforms we will apply to our images)\ndata_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[.5], std=[.5])\n])\n\ntrain_dataset = build_train_dataset(train_img_dir, img2label, transform = data_transform)","metadata":{"id":"9CdcixWp70yE","outputId":"1cbfda55-7a59-4be1-de1e-867d94986abe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instead of re-building the dataset everytime, we will just load in a pre-pickled Python object.","metadata":{"id":"YGvR2GQ5ko4Z"}},{"cell_type":"code","source":"import pickle\n\n# load in our pickled dataset\nwith open(os.path.join(data_folder_name, \"train_dataset.pkl\"), 'rb') as f:\n    train_dataset = pickle.load(f)","metadata":{"id":"TKm2-owHkqEB","execution":{"iopub.status.busy":"2024-07-29T16:24:34.565012Z","iopub.execute_input":"2024-07-29T16:24:34.565629Z","iopub.status.idle":"2024-07-29T16:24:34.600079Z","shell.execute_reply.started":"2024-07-29T16:24:34.565597Z","shell.execute_reply":"2024-07-29T16:24:34.599142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# format: [0_count, 1_count]\ndef count_each_class(sample_dataset):\n    total_datasets= len(sample_dataset)\n    unhealthy_count = len([each_item for each_item in sample_dataset if each_item[1] == 1])\n    #print(f\"total_dataset : {total_datasets}, healthy_count : {total_datasets- unhealthy_count}, unhealthy_count : {unhealthy_count}\")\n    return torch.tensor([(total_datasets- unhealthy_count), unhealthy_count])\n\ncount_each_class(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:24:37.738949Z","iopub.execute_input":"2024-07-29T16:24:37.739662Z","iopub.status.idle":"2024-07-29T16:24:38.488546Z","shell.execute_reply.started":"2024-07-29T16:24:37.739632Z","shell.execute_reply":"2024-07-29T16:24:38.487615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_class_weights(sample_dataset):\n    total_datasets= len(sample_dataset)\n    unhealthy_count = len([each_item for each_item in sample_dataset if each_item[1] == 1])\n    #print(f\"total_dataset : {total_datasets}, healthy_count : {total_datasets- unhealthy_count}, unhealthy_count : {unhealthy_count}\")\n    return torch.tensor([1./(total_datasets- unhealthy_count), 1/unhealthy_count])\n\nget_class_weights(train_dataset)","metadata":{"id":"Ypo-PaeVkt2Y","outputId":"bd517971-c7f9-43d6-d57c-3696d2655870","execution":{"iopub.status.busy":"2024-07-29T16:24:41.120530Z","iopub.execute_input":"2024-07-29T16:24:41.121352Z","iopub.status.idle":"2024-07-29T16:24:41.806982Z","shell.execute_reply.started":"2024-07-29T16:24:41.121323Z","shell.execute_reply":"2024-07-29T16:24:41.806015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_samples_weight(class_weights, sample_dataset):\n    samples_weight = torch.tensor([class_weights[int(each_item[1])] for each_item in sample_dataset])\n    return samples_weight\n    \n    #class_weights = get_class_weights(train_dataset)\n    #samples_weight = torch.tensor([class_weights[int(t)] for t in targets])\nget_samples_weight(get_class_weights(train_dataset), train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:25:11.139486Z","iopub.execute_input":"2024-07-29T16:25:11.139828Z","iopub.status.idle":"2024-07-29T16:25:12.490819Z","shell.execute_reply.started":"2024-07-29T16:25:11.139805Z","shell.execute_reply":"2024-07-29T16:25:12.489859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=1.0):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n    \n# Define transformations including scaling and shifting\ntransform0 = transforms.Compose([\n#     transforms.ToPILImage(),\n#     transforms.RandomHorizontalFlip(),\n#     transforms.RandomVerticalFlip(),\n#     transforms.RandomRotation(20),\n#     transforms.RandomAffine(0, scale=(0.8, 1.2), translate=(0.1, 0.1)),\n    transforms.ToTensor()\n])\n\ntransform1 = transforms.Compose([\n    transforms.ToPILImage(),\n    #transforms.RandomHorizontalFlip(),\n    #transforms.RandomVerticalFlip(),\n    transforms.RandomRotation(18),\n    transforms.RandomAffine(0, scale=(0.89, 1.12), translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    #transforms.Normalize(mean=[.5], std=[.5]),\n    #AddGaussianNoise(mean=0.0, std=0.1)\n])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T17:18:13.421043Z","iopub.execute_input":"2024-07-29T17:18:13.421732Z","iopub.status.idle":"2024-07-29T17:18:13.429864Z","shell.execute_reply.started":"2024-07-29T17:18:13.421700Z","shell.execute_reply":"2024-07-29T17:18:13.428813Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset[0][0].shape","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:35:45.692861Z","iopub.execute_input":"2024-07-29T16:35:45.693166Z","iopub.status.idle":"2024-07-29T16:35:45.700119Z","shell.execute_reply.started":"2024-07-29T16:35:45.693139Z","shell.execute_reply":"2024-07-29T16:35:45.699243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom imblearn.over_sampling import SMOTE, ADASYN, KMeansSMOTE, SVMSMOTE\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\n\nclass ResampledImageDataset(Dataset):\n    def __init__(self, images, labels, transform = None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = self.labels[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n    \n    \n# Example of extracting data from a PyTorch Dataset\ndef extract_data_from_dataset(dataset):\n    data = []\n    labels = []\n    for inputs, target in dataset:\n        data.append(inputs.numpy())\n        labels.append(target.numpy())\n    return np.array(data), np.array(labels)\n\n\ndef get_smote_dataset(dataset):\n    X_train, y_train = extract_data_from_dataset(train_dataset)\n    print(\"X train shape: \",X_train.shape )\n#     data_train, data_val, labels_train, labels_val = train_test_split(\n#         X_train, y_train, test_size = 0.2, random_state =42)\n    \n    # Flatten the images into 2D for smoting.\n    X_train_flattened = X_train.reshape(X_train.shape[0], -1)\n    \n    \n    # Apply SMOTE\n    smote = SMOTE(random_state=42)\n    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flattened, y_train)\n    \n    # Reshape back to original image dimensions\n    X_train_resampled = X_train_resampled.reshape(-1,  X_train.shape[2], X_train.shape[3], X_train.shape[1])\n    #X_val_resampled = data_val.reshape(-1, data_val.shape[2], data_val.shape[3], data_val.shape[1])\n    \n    # Convert to PyTorch tensors\n#     X_train_resampled_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n#     y_train_resampled_tensor = torch.tensor(y_train_resampled, dtype=torch.long)\n    \n    # Instantiate the train dataset\n    resampled_dataset_smote = ResampledImageDataset(X_train_resampled, y_train_resampled, transform = transform0)\n    resampled_dataset_augumented = ResampledImageDataset(X_train_resampled, y_train_resampled, transform = transform1)\n    \n    # validation dataset:\n    #validation_dataset = ResampledImageDataset(X_val_resampled, labels_val, transform = transform0)\n    \n    #for SVMSmote:\n     # Apply SMOTE\n    smote = SVMSMOTE(random_state=42)\n    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_flattened, y_train)\n    \n    # Reshape back to original image dimensions\n    X_train_resampled = X_train_resampled.reshape(-1,  X_train.shape[2], X_train.shape[3], X_train.shape[1])\n    #X_val_resampled = data_val.reshape(-1, data_val.shape[2], data_val.shape[3], data_val.shape[1])\n    \n    # Convert to PyTorch tensors\n#     X_train_resampled_tensor = torch.tensor(X_train_resampled, dtype=torch.float32)\n#     y_train_resampled_tensor = torch.tensor(y_train_resampled, dtype=torch.long)\n    \n    # Instantiate the train dataset\n    resampled_dataset_svmsmote = ResampledImageDataset(X_train_resampled, y_train_resampled, transform = transform0)\n    \n    \n    return resampled_dataset_smote + resampled_dataset_augumented","metadata":{"execution":{"iopub.status.busy":"2024-07-29T21:22:50.157230Z","iopub.execute_input":"2024-07-29T21:22:50.157900Z","iopub.status.idle":"2024-07-29T21:22:50.172057Z","shell.execute_reply.started":"2024-07-29T21:22:50.157869Z","shell.execute_reply":"2024-07-29T21:22:50.171083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# testing smote function:\nsmote_dataset = get_smote_dataset(train_dataset)\nlen(smote_dataset)\nget_class_weights(smote_dataset)\ncount_each_class(smote_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T21:22:54.095497Z","iopub.execute_input":"2024-07-29T21:22:54.096153Z","iopub.status.idle":"2024-07-29T21:23:24.157147Z","shell.execute_reply.started":"2024-07-29T21:22:54.096119Z","shell.execute_reply":"2024-07-29T21:23:24.156147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset), len(smote_dataset), len(validaton_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T21:23:36.365868Z","iopub.execute_input":"2024-07-29T21:23:36.366234Z","iopub.status.idle":"2024-07-29T21:23:36.373223Z","shell.execute_reply.started":"2024-07-29T21:23:36.366205Z","shell.execute_reply":"2024-07-29T21:23:36.372285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_each_class(train_dataset), count_each_class(smote_dataset), count_each_class(validaton_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-07-29T17:25:03.653916Z","iopub.execute_input":"2024-07-29T17:25:03.654386Z","iopub.status.idle":"2024-07-29T17:25:06.356299Z","shell.execute_reply.started":"2024-07-29T17:25:03.654343Z","shell.execute_reply":"2024-07-29T17:25:06.355382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.equal(train_dataset[0][0], smote_dataset[0][0])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:05.828981Z","iopub.execute_input":"2024-07-29T16:28:05.829302Z","iopub.status.idle":"2024-07-29T16:28:05.838117Z","shell.execute_reply.started":"2024-07-29T16:28:05.829278Z","shell.execute_reply":"2024-07-29T16:28:05.837086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to plot examples\ndef plot_examples(dataset, label, num_examples=5, indexes = None):\n    count = 0\n    if indexes:\n        fig, axes = plt.subplots(1, len(indexes), figsize=(15, 3))\n        for idx in indexes:\n            img = dataset[idx][0]\n            lbl = dataset[idx][1]\n            img_np = img.numpy().squeeze()\n            axes[count].imshow(img_np, cmap='gray')\n            axes[count].set_title(f'Label: {lbl}')\n            axes[count].axis('off')\n            count += 1\n        \n    else:\n        fig, axes = plt.subplots(1, num_examples, figsize=(15, 3))\n        for img, lbl in dataset:\n            if lbl == label:\n                img_np = img.numpy().squeeze()\n                axes[count].imshow(img_np, cmap='gray')\n                axes[count].set_title(f'Label: {lbl}')\n                axes[count].axis('off')\n                count += 1\n                if count == num_examples:\n                    break\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:28:07.561375Z","iopub.execute_input":"2024-07-29T16:28:07.562233Z","iopub.status.idle":"2024-07-29T16:28:07.571692Z","shell.execute_reply.started":"2024-07-29T16:28:07.562198Z","shell.execute_reply":"2024-07-29T16:28:07.570146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_examples(train_dataset, 1, num_examples=5, indexes = [0,400])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:05:57.833818Z","iopub.execute_input":"2024-07-29T19:05:57.834512Z","iopub.status.idle":"2024-07-29T19:05:58.074349Z","shell.execute_reply.started":"2024-07-29T19:05:57.834473Z","shell.execute_reply":"2024-07-29T19:05:58.073342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_examples(smote_dataset, 0, num_examples=5, indexes = [5,30000])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T21:23:47.098380Z","iopub.execute_input":"2024-07-29T21:23:47.099044Z","iopub.status.idle":"2024-07-29T21:23:47.264268Z","shell.execute_reply.started":"2024-07-29T21:23:47.099011Z","shell.execute_reply":"2024-07-29T21:23:47.263225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_examples(smote_dataset, 0, num_examples=5, indexes = [11000,13])","metadata":{"execution":{"iopub.status.busy":"2024-07-29T16:38:16.139846Z","iopub.execute_input":"2024-07-29T16:38:16.140221Z","iopub.status.idle":"2024-07-29T16:38:16.361142Z","shell.execute_reply.started":"2024-07-29T16:38:16.140192Z","shell.execute_reply":"2024-07-29T16:38:16.360211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Federated Learning","metadata":{"id":"HxM9JIbZIs-0"}},{"cell_type":"markdown","source":"In this competition, we will be dealing with a specialised form of machine learning called *federated learning* which you should have heard a *little* bit about in the days prior. To do this we will simulate a federated learning setup right here in this notebook using some simple classes and functions. These were designed to be very easy to use so during the competition you will not need to change much in this setup. It works by having Client objects who will act as our nodes which we will train on locally, and a Server object which acts as our centralised server containing the global model. The aggregation strategy will be fixed here to **FedAvg** for simplicity.\n\nQuick reminder, the equation for **FedAvg** [1] is the following:\n\n $$w^{t+1} = \\dfrac{1}{n}\\sum_{k=1}^{K} w_{k}^{t+1}$$\n\nEach term represents:\n* $w^{t+1}$: Updated global model parameters after round $t$.\n* $n$: Total number of clients.\n* $w_{k}^{t+1}$: Updated model parameters from client $k$ after local training.\n\nWhen sending the model from the Server to the Client we will do a local average instead of overwriting the local model weights (with the global model) to preserve local structure in the local models at the respective Clients (take a look at 'send_with_local_ave' within the Server class).","metadata":{"id":"HWhMT0MoIFLM"}},{"cell_type":"code","source":"client_weights = []\nclient_weights.append(clients[0].model.state_dict())","metadata":{"id":"OIuuA3PAEMak"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#!pip install autoPyTorch","metadata":{"id":"tS162Q5pFEot","outputId":"41fe2472-c8ba-47b6-e9f1-a6cb067e9bdd","execution":{"iopub.status.busy":"2024-07-29T18:59:52.850052Z","iopub.execute_input":"2024-07-29T18:59:52.851039Z","iopub.status.idle":"2024-07-29T18:59:52.855625Z","shell.execute_reply.started":"2024-07-29T18:59:52.851002Z","shell.execute_reply":"2024-07-29T18:59:52.854732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, WeightedRandomSampler\n\n\nclass Client:\n    \"\"\"\n    A class representing a client in a federated learning setup.\n\n    Attributes:\n    - device: The device on which the client's model and data will be loaded (CPU or CUDA).\n    - model: The initial model assigned to the client.\n    - data: The dataset assigned to the client.\n    - id: A unique identifier for the client.\n    \"\"\"\n    def __init__(self, initial_model, data, id, class_weight):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = copy.deepcopy(initial_model.to(self.device))\n        self.data = data\n        self.id = id\n        self.class_weight = torch.tensor(class_weight)\n        self.train_data_smote = get_smote_dataset(self.data)\n\n    def get_model_params(self):\n        \"\"\"\n        Returns the state dictionary of the client's model.\n\n        Returns:\n        - state_dict: The state dictionary of the client's model.\n        \"\"\"\n        return self.model.state_dict()\n\n\nclass Server:\n    \"\"\"\n    A class representing a central server in a federated learning setup.\n\n    Attributes:\n    - device: The device on which the server's global model will be loaded (CPU or CUDA).\n    - global_model: The global model maintained by the server.\n    - send_funcs: A dictionary mapping boolean flags to the corresponding send functions.\n    - send: The send function to be used (with or without local averaging) mimicking sending of weights from server to client.\n    \"\"\"\n    def __init__(self, model, local_weight_ave=True):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.global_model = copy.deepcopy(model.to(self.device))\n        self.send_funcs = {\n            True: self.send_with_local_ave,\n            False: self.send_without_local_ave\n        }\n        self.send = self.send_funcs[local_weight_ave]\n\n    def send_without_local_ave(self, client):\n        \"\"\"\n        Sends the global model to the client without local averaging.\n\n        Parameters:\n        - client: The client to which the global model will be sent.\n        \"\"\"\n        client.model.load_state_dict(self.global_model.state_dict())\n        print(f\"Sent model to Client {client.id}\")\n\n    def send_with_local_ave(self, client):\n        \"\"\"\n        Sends the global model to the client with local averaging.\n\n        Parameters:\n        - client: The client to which the averaged model will be sent.\n        \"\"\"\n        global_params = self.global_model.state_dict()\n        local_params = client.get_model_params()\n        averaged_params = {key: (global_params[key] + local_params[key]) / 2 for key in global_params.keys()}\n        client.model.load_state_dict(averaged_params)\n        print(f\"Sent model to Client {client.id}\")\n\n    def train(self, clients, gen_optimizer, loss_fn, num_epochs, batch_size=8, shuffle=True, lr=0.01):\n        \"\"\"\n        Trains the global model using data from multiple clients.\n\n        Parameters:\n        - clients: A list of Client objects.\n        - gen_optimizer: The optimizer generator function (basically a wrapper of a PyTorch optimiser object).\n        - loss_fn: The loss function to be used for training.\n        - num_epochs: The number of epochs for training.\n        - batch_size: The batch size for data loading (default is 8).\n        - shuffle: Whether to shuffle the data (default is True).\n        - lr: Learning rate for the optimizer (default is 0.01).\n        \"\"\"\n        client_weights = []\n        for client in clients:\n            self.send(client)\n            \n#             #WEighted sampling loggic:\n#             samples_weight = get_samples_weight(client.class_weight, client.data)\n#             print(samples_weight)\n#             sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement= True)\n            \n            train_data_loader = DataLoader(client.train_data_smote, batch_size=batch_size, shuffle = shuffle)\n            #validation_data_loader = DataLoader(client.validation_data, batch_size=batch_size, shuffle = False)\n            \n            optimizer = gen_optimizer(params=client.model.parameters(), lr=lr)\n            client.model.train()\n            print(f'Begin local training at Client {client.id}')\n            num_0 = 0\n            num_1 = 0\n            \n            train_losses = []\n            val_losses = []\n            train_accuracies = []\n            val_accuracies = []\n            \n            for epoch in range(num_epochs):\n                train_loss = 0.0\n                correct_train = 0\n                total_train = 0\n                \n                losses = []\n                for data, target in train_data_loader:\n                    num_0 += torch.sum(target == 0)\n                    num_1 += torch.sum(target == 1)\n                    data, target = data.to(self.device), target.to(self.device)\n                    class_weight = client.class_weight.to(self.device)\n                    optimizer.zero_grad()\n                    # print(f\"loaded data\")\n                    output = client.model(data)\n                    loss = loss_fn(output, target, class_weight)\n                    loss.backward()\n                    optimizer.step()\n                    \n                    \n                    losses.append(loss.clone().detach())\n                ave_loss = torch.mean(torch.tensor(losses))\n                print(f'Training loss, running average over batches, at end of epoch {epoch} is {ave_loss}')\n            print(f'End of local training at Client {client.id}')\n            client_weights.append(client.get_model_params())\n            print(\"num0 = \", num_0)\n            print(\"num1 = \", num_1)\n            print('------------------------------------------------------------------')\n        \n        avg_model_params = self.aggregate_weights(client_weights)\n        self.global_model.load_state_dict(avg_model_params)\n        \n    \n    def train_automl(self, clients, gen_optimizer, loss_fn, num_epochs, batch_size=8, shuffle=True, lr=0.01):\n        \"\"\"\n        Trains the global model using data from multiple clients.\n\n        Parameters:\n        - clients: A list of Client objects.\n        - gen_optimizer: The optimizer generator function (basically a wrapper of a PyTorch optimiser object).\n        - loss_fn: The loss function to be used for training.\n        - num_epochs: The number of epochs for training.\n        - batch_size: The batch size for data loading (default is 8).\n        - shuffle: Whether to shuffle the data (default is True).\n        - lr: Learning rate for the optimizer (default is 0.01).\n        \"\"\"\n        client_weights = []\n        for client in clients:\n            self.send(client)\n            \n#             #WEighted sampling loggic:\n#             samples_weight = get_samples_weight(client.class_weight, client.data)\n#             print(samples_weight)\n#             sampler = WeightedRandomSampler(samples_weight, len(samples_weight), replacement= True)\n            \n            train_data_loader = DataLoader(client.train_data_smote, batch_size=batch_size, shuffle = shuffle)\n            validation_data_loader = DataLoader(client.validation_data, batch_size=batch_size, shuffle = False)\n            \n            optimizer = gen_optimizer(params=client.model.parameters(), lr=lr)\n            client.model.train()\n            print(f'Begin local training at Client {client.id}')\n            num_0 = 0\n            num_1 = 0\n            \n            train_losses = []\n            val_losses = []\n            train_accuracies = []\n            val_accuracies = []\n            \n            for epoch in range(num_epochs):\n                train_loss = 0.0\n                correct_train = 0\n                total_train = 0\n                \n                losses = []\n                for data, target in train_data_loader:\n                    num_0 += torch.sum(target == 0)\n                    num_1 += torch.sum(target == 1)\n                    data, target = data.to(self.device), target.to(self.device)\n                    class_weight = client.class_weight.to(self.device)\n                    optimizer.zero_grad()\n                    # print(f\"loaded data\")\n                    output = client.model(data)\n                    loss = loss_fn(output, target, class_weight)\n                    loss.backward()\n                    optimizer.step()\n                    \n                    \n                    losses.append(loss.clone().detach())\n                ave_loss = torch.mean(torch.tensor(losses))\n                print(f'Training loss, running average over batches, at end of epoch {epoch} is {ave_loss}')\n            print(f'End of local training at Client {client.id}')\n            client_weights.append(client.get_model_params())\n            print(\"num0 = \", num_0)\n            print(\"num1 = \", num_1)\n            print('------------------------------------------------------------------')\n        \n        avg_model_params = self.aggregate_weights(client_weights)\n        self.global_model.load_state_dict(avg_model_params)\n        \n\n    def aggregate_weights(self, client_weights):\n        \"\"\"\n        Aggregates the weights from multiple clients using simple averaging.\n\n        Parameters:\n        - client_weights: A list of state dictionaries from clients.\n\n        Returns:\n        - avg_params: The averaged state dictionary.\n        \"\"\"\n        avg_params = {key: torch.zeros_like(param, dtype = torch.float32) for key, param in client_weights[0].items()}\n        for params in client_weights:\n            for key in params.keys():\n                avg_params[key] += params[key] / len(client_weights)\n        return avg_params\n\n    # need a nice test function here that incorporates all of your \"FAIR\" metrics\n    # def test(self, validation_dataset, batch_size=64, device=None):\n\n\ndef setup_federated_simulation_env(model, client_names, dataset, data_weights, local_weight_ave=True):\n    \"\"\"\n    Sets up the federated learning simulation environment.\n\n    Parameters:\n    - model: The initial global model.\n    - client_names: A list of names for the clients.\n    - dataset: The dataset to be split among the clients.\n    - data_weights: A list of weights specifying the proportion of data for each client.\n    - local_weight_ave: A boolean flag indicating whether to use local weight averaging (default is True).\n\n    Returns:\n    - central_server: The Server object.\n    - clients: A list of Client objects.\n    \"\"\"\n    print('---------- Setting up Federating Learning Environment ----------')\n    print('...')\n    print(\"length of dataset: \", len(dataset))\n    central_server = Server(model, local_weight_ave=local_weight_ave)\n    if len(client_names) != len(data_weights):\n        raise ValueError(\"Number of clients does not match proposed data splitting given data weights.\")\n    subset_datasets = split_dataset(dataset, data_weights)\n    class_weights = [get_class_weights(subset_dataset) for subset_dataset in subset_datasets]\n    clients = [Client(model, subset_dataset, client_name, class_weight) for client_name, subset_dataset, class_weight in zip(client_names, subset_datasets, class_weights)]\n    print('\\nClasss Weights for each client is : ', class_weights)\n    print('\\n---------- Finished Setting up Federating Learning Environment ----------')\n    print()\n    return central_server, clients\n\n\n\n# helper function to split the datasets\nimport numpy as np\nfrom torch.utils.data import Subset\n\ndef split_dataset(dataset, data_weights):\n    \"\"\"\n    Splits a dataset into multiple subsets according to specified weights.\n\n    Parameters:\n    - dataset: The original dataset to be split.\n    - data_weights: A list of weights specifying the proportion of each subset.\n                    The sum of data_weights should be 1.\n\n    Returns:\n    - A list of dataset objects, each corresponding to a subset of the original dataset.\n    \"\"\"\n    if not np.isclose(np.sum(data_weights), 1):\n        raise ValueError('Invalid data_weights, do not sum to 1.')\n    total_samples = len(dataset)\n    indices = np.arange(total_samples)\n    np.random.shuffle(indices)\n\n    split_indices = []\n    current_idx = 0\n    for weight in data_weights:\n        split_size = int(np.floor(weight * total_samples))\n        split_indices.append(indices[current_idx:current_idx + split_size])\n        current_idx += split_size\n\n    # Ensure that all samples are included (add the remaining samples to the last subset)\n    if current_idx < total_samples:\n        split_indices[-1] = np.concatenate((split_indices[-1], indices[current_idx:]), axis=0)\n\n    # Create the subset datasets\n    subset_datasets = [Subset(dataset, indices) for indices in split_indices]\n\n    return subset_datasets","metadata":{"id":"tXEv9fbhFK1g","execution":{"iopub.status.busy":"2024-07-29T19:37:34.287669Z","iopub.execute_input":"2024-07-29T19:37:34.288032Z","iopub.status.idle":"2024-07-29T19:37:34.329443Z","shell.execute_reply.started":"2024-07-29T19:37:34.288000Z","shell.execute_reply":"2024-07-29T19:37:34.328492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Federated setup is designed such that the only \"things\" to vary is the model, the optimiser and the loss function. When we eventually get to the training run it will be clear how to do this but before that let's define some basic models to use.","metadata":{"id":"A7lC0AiVAfc1"}},{"cell_type":"markdown","source":"Here we have included some very simple models that you can start off with.","metadata":{"id":"xE0OvVUXp9SD"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# simple MLP\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_dim=28*28*1, hidden_dim=128, output_dim=2):\n        super(SimpleMLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten the input tensor\n        x = F.relu(self.fc1(x))\n        out = self.fc2(x)\n        return out\n\n# Logistic Regression\nclass LogisticRegressor(nn.Module):\n    def __init__(self, input_dim=28*28*1, output_dim=2):\n        super(LogisticRegressor, self).__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten the input tensor\n        out = self.fc(x)\n        return out\n\n\n# SimpleCNN architecture\nclass SimpleCNN(nn.Module):\n    def __init__(self, num_classes=2):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=28, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=28, out_channels=64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(in_features=64*7*7, out_features=128)\n        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.pool(self.relu(self.conv1(x)))\n        x = self.pool(self.relu(self.conv2(x)))\n        x = x.view(-1, 64*7*7) # this flattens the tensor for FC layer\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nimport torch\nimport torch.nn as nn\n\nclass EnhancedCNN(nn.Module):\n    def __init__(self, num_classes=2):\n        super(EnhancedCNN, self).__init__()\n        # First convolutional layer\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n        self.bn1 = nn.BatchNorm2d(32)  # Batch normalization for the first layer\n        # Second convolutional layer\n        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)  # Batch normalization for the second layer\n        # Third convolutional layer\n        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)  # Batch normalization for the third layer\n        # Fourth convolutional layer\n        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n        self.bn4 = nn.BatchNorm2d(256)  # Batch normalization for the fourth layer\n        # Max pooling layer\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        # Dropout layer\n        self.dropout = nn.Dropout(0.5)  # Dropout with 50% probability\n        # Calculate the input size for the first fully connected layer\n        self._to_linear = None\n        self.conv_layers = nn.Sequential(\n            self.conv1, self.bn1, nn.ReLU(), self.pool,\n            self.conv2, self.bn2, nn.ReLU(), self.pool,\n            self.conv3, self.bn3, nn.ReLU(), self.pool,\n            self.conv4, self.bn4, nn.ReLU(), self.pool\n        )\n        self._get_conv_output_size()\n        # First fully connected layer\n        self.fc1 = nn.Linear(in_features=self._to_linear, out_features=512)\n        self.fc2 = nn.Linear(in_features=512, out_features=256)\n        self.fc3 = nn.Linear(in_features=256, out_features=num_classes)\n        # ReLU activation\n        self.relu = nn.ReLU()\n\n    def _get_conv_output_size(self):\n        with torch.no_grad():\n            x = torch.zeros(1, 1, 28, 28)  # Assuming input image size is 28x28\n            x = self.conv_layers(x)\n            self._to_linear = x.view(x.size(0), -1).size(1)\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten the tensor for the fully connected layer\n        x = self.dropout(self.relu(self.fc1(x)))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nmodel = EnhancedCNN(num_classes=2)\nprint(model)","metadata":{"id":"YpG4hhwaBHwU","outputId":"e9918066-ac77-4f35-930e-7d3a9c55eadc","execution":{"iopub.status.busy":"2024-07-29T19:37:36.382424Z","iopub.execute_input":"2024-07-29T19:37:36.382785Z","iopub.status.idle":"2024-07-29T19:37:36.419246Z","shell.execute_reply.started":"2024-07-29T19:37:36.382758Z","shell.execute_reply":"2024-07-29T19:37:36.418437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's initialize our Federated Learning simulation and perform a demo training run to see how it all works together. Pay attention particularly to the 'gen_optimizer' and the 'criterion' functions. In our simulation, we are training locally, and because some optimizers, like Adam, leverage features like gradient history, we need to initialize our optimizers at the Client. To achieve this, we generate the optimizer locally when we train, which is slightly different from simply feeding in a PyTorch optimizer.","metadata":{"id":"jhCHLdSxC_BC"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass CustomResNet18(nn.Module):\n    def __init__(self):\n        super(CustomResNet18, self).__init__()\n        # Load the pre-trained ResNet18 model\n        self.resnet18 = models.resnet18(pretrained=False)\n        \n        # Replace the first convolutional layer to accept grayscale images (1 channel)\n        self.resnet18.conv1 = nn.Conv2d(\n            in_channels=1, \n            out_channels=self.resnet18.conv1.out_channels, \n            kernel_size=self.resnet18.conv1.kernel_size, \n            stride=self.resnet18.conv1.stride, \n            padding=self.resnet18.conv1.padding, \n            bias=self.resnet18.conv1.bias\n        )\n        \n        # Replace the final fully connected layer to output 2 classes\n        num_ftrs = self.resnet18.fc.in_features\n        self.resnet18.fc = nn.Linear(num_ftrs, 2)\n        \n        # Reinitialize the weights of the modified layer\n        #nn.init.kaiming_normal_(self.resnet18.conv1.weight, mode='fan_out', nonlinearity='relu')\n\n    def forward(self, x):\n        return self.resnet18(x)\n\n# Instantiate the model\nmodel = CustomResNet18()\n\n# Check the model architecture\nprint(model)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-29T19:37:40.461533Z","iopub.execute_input":"2024-07-29T19:37:40.461919Z","iopub.status.idle":"2024-07-29T19:37:40.660819Z","shell.execute_reply.started":"2024-07-29T19:37:40.461888Z","shell.execute_reply":"2024-07-29T19:37:40.659892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set a seed DO NOT change this\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ntorch.cuda.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nmodel = LogisticRegressor()\nglobal_model = model\n\n\nclient_names = ['Hospital 1', 'Hospital 2','Hospital 3', 'Hospital 4']\ndata_weights = [0.25, 0.25, 0.25, 0.25]\nserver, clients =  setup_federated_simulation_env(model = global_model, client_names = client_names, dataset = train_dataset, data_weights = data_weights, local_weight_ave = True)\n\n# Define optimizer and loss function\ndef gen_optimizer(params, lr):\n    return torch.optim.SGD(params=params, lr=lr)\n\ndef criterion(input, target, class_weight):\n    loss_fn = torch.nn.CrossEntropyLoss()\n    #loss_fn = torch.nn.CrossEntropyLoss(weight = class_weight)\n    return loss_fn(input, target)\n\n# Train using Federated Averaging\nserver.train(clients, gen_optimizer, loss_fn = criterion, num_epochs=200, lr = 0.001) # can quickly see the effect of a large learning rate","metadata":{"id":"8I8xT1ydAi5t","outputId":"4a1c096c-9e96-4a2d-8f68-73cffa2540eb","execution":{"iopub.status.busy":"2024-07-30T04:23:39.624805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📊 Evaluation","metadata":{"id":"VZrkfDmXMKzb"}},{"cell_type":"markdown","source":"A focus of this competition is to learn about different metrics that are used in classification, to try mitigate bias or imbalances in data. Lets look at some more \"conventional\" metrics first.","metadata":{"id":"vc_ycq-_MO7x"}},{"cell_type":"markdown","source":"When evaluating the performance of a binary classification model, several metrics are commonly used. Below, we explain and provide the formulas for some of these key metrics: Accuracy, Confusion Matrix, Sensitivity, and Specificity.\n\n## 1. Accuracy\n\n**Accuracy** is the ratio of correctly predicted instances to the total instances. It is a measure of the overall effectiveness of the classifier.\n\n**Formula:**\n\n$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n\nWhere:\n- \\( TP \\) = True Positives\n- \\( TN \\) = True Negatives\n- \\( FP \\) = False Positives\n- \\( FN \\) = False Negatives\n\n## 2. Confusion Matrix\n\nA **Confusion Matrix** is a table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n\n\n\\begin{array}{c|c|c}\n\\text{Actual / Predicted} & \\text{Positive} & \\text{Negative} \\\\\n\\hline\n\\text{Positive} & TP & FN \\\\\n\\text{Negative} & FP & TN \\\\\n\\end{array}\n\n\n## 3. Sensitivity (Recall)\n\n**Sensitivity**, also known as **Recall**, measures the proportion of actual positives that are correctly identified by the model. It focuses on the ability of the classifier to find all the positive samples.\n\n**Formula:**\n\n\n$$ \\text{Sensitivity} = \\frac{TP}{TP + FN} $$\n\n\n## 4. Specificity\n\n**Specificity** measures the proportion of actual negatives that are correctly identified by the model. It focuses on the ability of the classifier to find all the negative samples.\n\n**Formula:**\n\n\n$$ \\text{Specificity} = \\frac{TN}{TN + FP} $$\n\n\n## Summary\n\nThese metrics help in understanding the performance of a binary classification model from different perspectives. While accuracy gives a general measure of performance, the confusion matrix provides a detailed breakdown, and sensitivity and specificity offer insights into how well the model performs on the positive and negative classes, respectively.\n","metadata":{"id":"fZ0pf2_pOS4c"}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import label_binarize\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix\n\ndef get_accuracy(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes overall accuracy for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 128).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - overall_accuracy: The overall accuracy over the entire dataset.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n\n    # Compute overall accuracy\n    overall_accuracy = (all_outputs.argmax(dim=1) == all_targets).float().mean().item()\n\n    return overall_accuracy\n\ndef get_confusion_matrix(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes and displays the confusion matrix for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 64).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - cm: Confusion matrix\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    binarized_targets = (all_targets == 1).to(torch.int)\n    binarized_preds = (all_outputs.argmax(dim=1) == 1).to(torch.int)\n    cm = confusion_matrix(binarized_targets, binarized_preds)\n\n    # Plot confusion matrix\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix for Predicting Disease')\n    plt.show()\n\n    return cm\n\ndef get_sensitivity(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes the sensitivity metric for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 1228).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - sensitivity: The sensitivity metric.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    binarized_targets = (all_targets == 1).to(torch.int)\n    binarized_preds = (all_outputs.argmax(dim=1) == 1).to(torch.int)\n    cm = confusion_matrix(binarized_targets, binarized_preds)\n\n    # Extract True Positives, False Positives, True Negatives, False Negatives\n    TP = cm[1, 1]\n    FP = cm[0, 1]\n    TN = cm[0, 0]\n    FN = cm[1, 0]\n\n    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n    return sensitivity\n\ndef get_specificity(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes the specificity metric for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 1228).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - specificity: The specificity metric.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    binarized_targets = (all_targets == 1).to(torch.int)\n    binarized_preds = (all_outputs.argmax(dim=1) == 1).to(torch.int)\n    cm = confusion_matrix(binarized_targets, binarized_preds)\n\n    # Extract True Positives, False Positives, True Negatives, False Negatives\n    TP = cm[1, 1]\n    FP = cm[0, 1]\n    TN = cm[0, 0]\n    FN = cm[1, 0]\n    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n\n    return specificity\n","metadata":{"id":"qXymqADVM4W2","execution":{"iopub.status.busy":"2024-07-29T17:16:06.555413Z","iopub.execute_input":"2024-07-29T17:16:06.555850Z","iopub.status.idle":"2024-07-29T17:16:06.584485Z","shell.execute_reply.started":"2024-07-29T17:16:06.555819Z","shell.execute_reply":"2024-07-29T17:16:06.583626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets test this on one of our clients data.","metadata":{"id":"S-Q7wh2cPq7V"}},{"cell_type":"code","source":"# lets get the accuracy\nclient0_data_accuracy = get_accuracy(clients[0].data, server.global_model)\nclient1_data_accuracy = get_accuracy(clients[1].data, server.global_model)\nclient2_data_accuracy = get_accuracy(clients[2].data, server.global_model)\nclient3_data_accuracy = get_accuracy(clients[3].data, server.global_model)\nprint(client0_data_accuracy, client1_data_accuracy, client2_data_accuracy, client3_data_accuracy)","metadata":{"id":"xtMRvqQfPuH4","outputId":"6ca27185-9f48-4b9c-f8cc-58319238a088","execution":{"iopub.status.busy":"2024-07-29T21:20:19.410964Z","iopub.execute_input":"2024-07-29T21:20:19.411630Z","iopub.status.idle":"2024-07-29T21:20:20.114248Z","shell.execute_reply.started":"2024-07-29T21:20:19.411592Z","shell.execute_reply":"2024-07-29T21:20:20.113222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have only trained our model for 5 epochs but we are already getting 79\\% accuracy. It appears that this is a fairly easy problem for us but before we simply run a few more epochs and close shop let's take a closer look by observing the 'confusion matrix'.","metadata":{"id":"DxNJ9NY1PySp"}},{"cell_type":"code","source":"get_confusion_matrix(clients[0].data, server.global_model)","metadata":{"id":"DsKIWneZQQ-l","outputId":"6b53f49e-044b-452f-bd26-656ca909b8aa","execution":{"iopub.status.busy":"2024-07-29T21:20:23.109544Z","iopub.execute_input":"2024-07-29T21:20:23.110235Z","iopub.status.idle":"2024-07-29T21:20:23.510072Z","shell.execute_reply.started":"2024-07-29T21:20:23.110203Z","shell.execute_reply":"2024-07-29T21:20:23.509107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This paints a clearer picture, even though our accuracy is high we can see that this is just a consequence of an imbalance in the data. You can see we do not predict a single diseased example of a lung correctly. The model is simply guessing \"healthy\" for every single sample (corresponding to a negative prediction for us). This is a common problem with metrics such as accuracy for data which is highly imbalanced. Let's take a look at the specificity and sensitivity metrics next.","metadata":{"id":"hhmiBsqtQUyp"}},{"cell_type":"code","source":"specificity = get_specificity(clients[0].data, clients[0].model)\nsensitivity = get_sensitivity(clients[0].data, clients[0].model)\n\nprint(f'The specificity for {clients[0].id} is {specificity}')\nprint(f'The sensitivity for {clients[0].id} is {sensitivity}')","metadata":{"id":"bw01qOmIRPSS","outputId":"de609a31-926a-4eaa-b84b-50bfb68a87e4","execution":{"iopub.status.busy":"2024-07-29T18:33:51.535438Z","iopub.execute_input":"2024-07-29T18:33:51.535825Z","iopub.status.idle":"2024-07-29T18:33:51.895338Z","shell.execute_reply.started":"2024-07-29T18:33:51.535788Z","shell.execute_reply":"2024-07-29T18:33:51.894471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client0_specificity = get_specificity(clients[0].data, server.global_model)\nclient1_specificity = get_specificity(clients[1].data, server.global_model)\nclient2_specificity = get_specificity(clients[2].data, server.global_model)\nclient3_specificity = get_specificity(clients[3].data, server.global_model)\n\nclient0_sensitivity = get_sensitivity(clients[0].data, server.global_model)\nclient1_sensitivity = get_sensitivity(clients[1].data, server.global_model)\nclient2_sensitivity = get_sensitivity(clients[2].data, server.global_model)\nclient3_sensitivity = get_sensitivity(clients[3].data, server.global_model)\n\n\nprint(client0_specificity, client1_specificity, client2_specificity, client3_specificity)\nprint(client0_sensitivity, client1_sensitivity, client2_sensitivity, client3_sensitivity)","metadata":{"id":"9GKM_pVWu2Fu","outputId":"007eec1b-198e-42fc-aae1-89a2aade773b","execution":{"iopub.status.busy":"2024-07-29T21:20:30.039016Z","iopub.execute_input":"2024-07-29T21:20:30.039373Z","iopub.status.idle":"2024-07-29T21:20:31.453871Z","shell.execute_reply.started":"2024-07-29T21:20:30.039345Z","shell.execute_reply":"2024-07-29T21:20:31.452789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you look up at the definitions this makes sense. Specificity is how many negative samples of the total number of negative samples we capture which is everything because our model simply predicts negative for everythig. Sensitivity takes a look at how many positive samples of the total positive samples we are able to capture. Idealy we would like some metrics that are able to account for these imbalances in a single metric.","metadata":{"id":"few6RuX2Rtak"}},{"cell_type":"markdown","source":"Let's introduce some new measures that are more robust and can better account for imbalances in data. These metrics and more recommendations can be found in this detailed Nature article [2].","metadata":{"id":"S2EcmimjT5Yf"}},{"cell_type":"markdown","source":"## 5. Balanced Accuracy\n\n**Balanced Accuracy** is the average of sensitivity and specificity. It is useful when the classes are imbalanced.\n\n**Formula:**\n\n\n$$ \\text{Balanced Accuracy} = \\frac{\\text{Sensitivity} + \\text{Specificity}}{2} $$\n\n## 6. Area Under the Receiver Operating Characteristic Curve (AUROC)\n\n**AUROC** is a performance measurement for classification problems at various threshold settings. ROC is a probability curve, and AUROC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes.\n\n### Receiver Operating Characteristic (ROC) Curve\n\nThe **ROC Curve** is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity) at various threshold settings (a threshold being a probablility value above which we consider a sample to belong to a certain class). It shows the trade-off between sensitivity and specificity.\n\n- **True Positive Rate (TPR)**: Same as Sensitivity\n- **False Positive Rate (FPR)**: $$ \\text{FPR} = \\frac{FP}{FP + TN} $$\n\nEach point on the curve corresponds to a FPR vs TPR for given probablity threshold [3].\n\n![ROCCurve.svg](data:image/svg+xml;base64,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)\n\n\n\n\n**AUROC** is the area under the ROC curve. The higher the AUROC, the better the model is at distinguishing between the positive and negative classes with perfect value of AUROC is 1. I know it can be quite tricky to grasp this concept if it is the first time you are seeing it. I would recommend reading this [blog](https://mlu-explain.github.io/roc-auc/) [4] which gives a pretty good explanation. It will also be more evident when looking at some concrete examples.\n\n\n## Summary 2\n\n Balanced accuracy and AUROC are metrics that are better adapted to dealing with imbalanced data. Balanced accuracy directly tackles the imbalance by averaging specificity and sensisitivity whilst AUROC assesses the model's overall ability to distinguish between classes i.e. how well it \"ranks\" the probabilities.\n","metadata":{"id":"RW5B1p8QTCRC"}},{"cell_type":"markdown","source":"Let's take a look at an example","metadata":{"id":"oQgR4YEdWmvt"}},{"cell_type":"code","source":"def get_balanced_accuracy(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes the balanced accuracy for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 1228).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - balanced_accuracy: The average balanced accuracy over all classes.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n\n    binarized_targets = (all_targets == 1).to(torch.int)\n    binarized_preds = (all_outputs.argmax(dim=1) == 1).to(torch.int)\n    cm = confusion_matrix(binarized_targets, binarized_preds)\n\n    # Extract True Positives, False Positives, True Negatives, False Negatives\n    TP = cm[1, 1]\n    FP = cm[0, 1]\n    TN = cm[0, 0]\n    FN = cm[1, 0]\n\n    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n    balanced_acc = (sensitivity + specificity) / 2\n\n\n    # balanced_accuracy = sum(class_balanced_accuracies) / num_classes\n    return balanced_acc\n\n# NB change the doc string here because you are no longer doing multi-class\ndef get_auroc_plot_roc(validation_dataset, model, batch_size=128, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n    \"\"\"\n    Calculate the AUROC and plot the ROC curve for a binary classification problem.\n\n    Parameters:\n    validation_dataset (Dataset): The validation dataset.\n    model (nn.Module): The trained model.\n    batch_size (int): The batch size for data loading.\n    device (str): The device to run the model on ('cpu' or 'cuda').\n\n    Returns:\n    roc_auc: AUROC value.\n    \"\"\"\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Create a DataLoader for the validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    # Lists to store true labels and predicted probabilities\n    all_labels = []\n    all_probs = []\n\n    # Disable gradient calculation for validation\n    with torch.no_grad():\n        for batch in val_loader:\n            # Move batch to the specified device\n            inputs, labels = batch\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Get model outputs (logits)\n            logits = model(inputs)\n\n            # Apply softmax to get probabilities\n            probabilities = F.softmax(logits, dim=1)\n\n            # Store labels and probabilities\n            all_labels.append(labels.cpu().numpy())\n            all_probs.append(probabilities.cpu().numpy())\n\n    # Concatenate all batches\n    all_labels = np.concatenate(all_labels)\n    all_probs = np.concatenate(all_probs)\n    all_probs = all_probs[:,1] # you need to only look at a single class for this metric\n\n\n    # Calculate ROC curve and AUROC\n\n    plt.figure(figsize=(12, 8))\n    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n    roc_auc = auc(fpr, tpr)\n\n    plt.plot(fpr, tpr, lw=2, label=f' ROC model predictor (area = {roc_auc:.2f})')\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=2, label = 'ROC random predictor (area = 0.5)')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curves')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show()\n    return roc_auc","metadata":{"id":"pk3zjxz-RC7H","execution":{"iopub.status.busy":"2024-07-29T17:16:39.027941Z","iopub.execute_input":"2024-07-29T17:16:39.028615Z","iopub.status.idle":"2024-07-29T17:16:39.046466Z","shell.execute_reply.started":"2024-07-29T17:16:39.028576Z","shell.execute_reply":"2024-07-29T17:16:39.045538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_balanced_accuracy(clients[0].data, clients[0].model)","metadata":{"id":"0OgrI1-qOMrH","outputId":"bd79c4d1-1ec5-4d20-b228-c3dded85662d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"client0_balanced_accuracy = get_balanced_accuracy(clients[0].data, server.global_model)\nclient1_balanced_accuracy = get_balanced_accuracy(clients[1].data, server.global_model)\nclient2_balanced_accuracy = get_balanced_accuracy(clients[2].data, server.global_model)\nclient3_balanced_accuracy = get_balanced_accuracy(clients[3].data, server.global_model)\nprint(client0_balanced_accuracy, client1_balanced_accuracy, client2_balanced_accuracy, client3_balanced_accuracy)","metadata":{"id":"3oV-95DGvfN7","outputId":"23751ff5-4f87-4ed3-a5b0-d76ff730adf8","execution":{"iopub.status.busy":"2024-07-29T21:20:42.119918Z","iopub.execute_input":"2024-07-29T21:20:42.120279Z","iopub.status.idle":"2024-07-29T21:20:42.844282Z","shell.execute_reply.started":"2024-07-29T21:20:42.120250Z","shell.execute_reply":"2024-07-29T21:20:42.843333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a much more reasonable metric, which reflects a \"truer\" performance of our model for our purposes. It is very easy to intuitivley see how we get this value. We get 100\\% of the negative label healthy samples but 0\\% of the positive label unhealthy samples so we are left with an average performance of 50\\%.","metadata":{"id":"Ikghx_8GXKvD"}},{"cell_type":"markdown","source":"Now let's take a look at the ROC curve and the Area under this curve.","metadata":{"id":"jGHosabyXN49"}},{"cell_type":"code","source":"get_auroc_plot_roc(clients[0].data, clients[0].model)","metadata":{"id":"iSrWaUc8XHnH","outputId":"72391186-0ce1-4525-9557-348df89d360d","execution":{"iopub.status.busy":"2024-07-29T19:31:02.969472Z","iopub.execute_input":"2024-07-29T19:31:02.969869Z","iopub.status.idle":"2024-07-29T19:31:03.391598Z","shell.execute_reply.started":"2024-07-29T19:31:02.969838Z","shell.execute_reply":"2024-07-29T19:31:03.390753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This also appears to give us a more \"sane\" performance metric as 0.60 is considerably less that the previous 0.79 we were getting with just accuracy.  ","metadata":{"id":"3omytAYGXVTh"}},{"cell_type":"markdown","source":"Feel free to use these functions to help you assess the performance of your model as you make submissions to help guide you along your fine-tuning process.","metadata":{"id":"oYL8B13fYHMR"}},{"cell_type":"code","source":"with mlflow.start_run() as run:\n  print(\"Logging Pytorch Model\")\n  mlflow.pytorch.log_model(\n      server.global_model, \"pytorch-model\", registered_model_name = \"SimpleCNN\"\n  )\n  model_uri = \"runs:/{}/pytorch-model\".format(run.info.run_id)\n  print(\"Model saved in run %s\" % run.info.run_id)\n  print(f\"Model URI : {model_uri}\")","metadata":{"id":"b7h3O1h_qBVV","outputId":"aba441f1-c879-4474-84e6-62a9b1a9e5f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"7zvlDzbUr0wh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kaggle Submission","metadata":{"id":"_m543wxXZAub"}},{"cell_type":"markdown","source":"Finally we need to create a submission for our Kaggle competition which will be a csv file containing the predictions of our test images. We first define some classes and functions to help with this.","metadata":{"id":"3Yuea1a7wu4T"}},{"cell_type":"code","source":"# from inspect import isgetsetdescriptor\n# Create a PyTorch Dataset\nclass ChestMNISTDatasetTest(Dataset):\n    \"\"\"\n    This class is used to turn our dataset into a PyTorch-supported dataset.\n\n    Parameters:\n    - images: The images array from the MedMNIST dataset.\n    - labels: The corresponding labels array for each image.\n    - transform: Optional transformation to be applied to the images.\n    \"\"\"\n    def __init__(self, images, ids, transform=None):\n        self.images = images\n        self.ids = ids\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        id = self.ids[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, id\n\ndef build_test_dataset(img_dir, transform=None):\n    # run through img directory store images as np.array in images array. Simultaneously store labels in numpy array, using img2label. Then convert labels to tensor (or you can do this in the Dataset class).\n    print('---------- Building Test Dataset ----------')\n    images = []\n    ids = []\n    img_fnames = os.listdir(img_dir)\n    total_images = len(img_fnames)\n    step = total_images // 10  # 10% step\n\n    for i, img_fname in enumerate(img_fnames):\n        img_path = os.path.join(img_dir, img_fname)\n        img = np.array(Image.open(img_path))\n        id = img_fname\n        images.append(img)\n        ids.append(id)\n\n        # Print progress\n        if (i + 1) % step == 0:\n            progress = (i + 1) // step * 10\n            print(f'{\"-\" * (progress // 10)} {progress}% complete ')\n\n    print('---------- Finished Building Test Dataset ----------')\n    print()\n    return ChestMNISTDatasetTest(images, ids, transform=transform)\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport pandas as pd\n\n\ndef gen_test_df(global_model, test_dataset, device, batch_size=32):\n    # Set the model to evaluation mode\n    global_model.eval()\n\n    # Create a DataLoader for the test dataset\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Lists to store ids and predicted labels\n    all_ids = []\n    all_outputs = []\n\n    total_batches = len(test_loader)\n    step = total_batches // 10  # 10% step\n\n    # Disable gradient calculation for evaluation\n    print('---------- Inference on Test Dataset ----------')\n    with torch.no_grad():\n        for i, batch in enumerate(test_loader):\n            # Move batch to the specified device\n            inputs, input_ids = batch\n            inputs = inputs.to(device)\n\n            # Get model outputs (logits)\n            logits = global_model(inputs)\n\n            # Get the predicted labels\n            predicted_labels = logits.argmax(dim=1).cpu().numpy()\n\n            # Append ids and outputs to lists\n            all_ids.extend(list(input_ids))\n            all_outputs.extend(predicted_labels)\n\n            # Print progress\n            if (i + 1) % step == 0:\n                progress = (i + 1) // step * 10\n                print(f'{\"-\" * (progress // 10)} {progress}% complete')\n    print('---------- Completed Inference on Test Dataset ----------')\n    print()\n    # Create a DataFrame with the ids and predicted labels\n    results_df = pd.DataFrame({\n        'ID': all_ids,\n        'label': all_outputs\n    })\n    results_df_sorted = results_df.sort_values(by=['ID'], ascending=True)\n    return results_df_sorted\n\n","metadata":{"id":"gKGCscqFMShm","execution":{"iopub.status.busy":"2024-07-29T19:31:11.959882Z","iopub.execute_input":"2024-07-29T19:31:11.960234Z","iopub.status.idle":"2024-07-29T19:31:11.976903Z","shell.execute_reply.started":"2024-07-29T19:31:11.960206Z","shell.execute_reply":"2024-07-29T19:31:11.975863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we use the above to generate the csv which we will use to make our submission. We first load in our test dataset.","metadata":{"id":"rq9l5AFvYyYo"}},{"cell_type":"code","source":"# preprocessing\ndata_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[.5], std=[.5])\n])\n\ntest_dataset = build_test_dataset(test_img_dir, transform = data_transform)","metadata":{"id":"bbBQ4rzAN0J9","outputId":"e45b03e7-4185-45c3-a135-e05a1cdfc190"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As before, for ease of use, we have the test_dataset pre-pickled so you can just load it in.","metadata":{"id":"U40NxMgWAgMV"}},{"cell_type":"code","source":"import pickle\n\n# load in pickled test dataset\nwith open(os.path.join(data_folder_name, \"test_dataset.pkl\"), 'rb') as f:\n    test_dataset = pickle.load(f)","metadata":{"id":"xd9-9CduOKZb","execution":{"iopub.status.busy":"2024-07-29T19:31:15.514851Z","iopub.execute_input":"2024-07-29T19:31:15.515207Z","iopub.status.idle":"2024-07-29T19:31:15.531242Z","shell.execute_reply.started":"2024-07-29T19:31:15.515178Z","shell.execute_reply":"2024-07-29T19:31:15.530526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Now we use our test_dataset object and our global model to generate our .csv submission file. **Note** during the competition when you are making submission you can just keep re-running the below block there is nothing you need to change.","metadata":{"id":"dIRLwj4jAUCe"}},{"cell_type":"code","source":"output_data_folder_name = \"/kaggle/working\"\nglobal_model = server.global_model\ndevice = server.device\nresults_df = gen_test_df(server.global_model, test_dataset, device, batch_size=32)\n\nresults_df.to_csv( os.path.join( output_data_folder_name, \"sample_submission.csv\"), index=False) # save csv the chosen directory.","metadata":{"id":"W8smq30MSAEh","outputId":"9f4b8038-4e49-4993-82d6-ae9c58d82be8","execution":{"iopub.status.busy":"2024-07-29T21:20:58.810264Z","iopub.execute_input":"2024-07-29T21:20:58.811004Z","iopub.status.idle":"2024-07-29T21:20:58.999224Z","shell.execute_reply.started":"2024-07-29T21:20:58.810972Z","shell.execute_reply":"2024-07-29T21:20:58.998314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And that's a wrap! You should have all the tools you need to start playing around with your own models and begin making your own submissions. Good luck!","metadata":{"id":"HtUWeVNh2Xfb"}},{"cell_type":"markdown","source":"# References\n[1]   **(FedAvg)** McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017, April). Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics (pp. 1273-1282). PMLR.\n\n[2] Maier-Hein, L., Reinke, A., Godau, P. et al. Metrics reloaded: recommendations for image analysis validation. Nat Methods 21, 195–212 (2024). https://doi.org/10.1038/s41592-023-02151-z\n\n[3] Google. “Classification: ROC Curve and AUC  |  Machine Learning Crash Course.” Google Developers, 2019, developers.google.com/machine-learning/crash-course/classification/roc-and-auc.\n\n[4] Wilber, Jared. “ROC and AUC.” MLU-Explain, mlu-explain.github.io/roc-auc/.\n\n\n","metadata":{"id":"SXdF6A2wJxEp"}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-28T04:39:35.162859Z","iopub.execute_input":"2024-07-28T04:39:35.163264Z","iopub.status.idle":"2024-07-28T04:39:36.526922Z","shell.execute_reply.started":"2024-07-28T04:39:35.163233Z","shell.execute_reply":"2024-07-28T04:39:36.525726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n\nclass ImbalancedDataset(Dataset):\n    def __init__(self, data, targets):\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx], self.targets[idx]\n\n# Example data\ndata = torch.randn(1000, 10)\ntargets = torch.cat((torch.zeros(900), torch.ones(100)))  # Imbalanced targets\n\ndataset = ImbalancedDataset(data, targets)\nlen(dataset)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:20:34.919567Z","iopub.execute_input":"2024-07-28T10:20:34.920755Z","iopub.status.idle":"2024-07-28T10:20:34.935169Z","shell.execute_reply.started":"2024-07-28T10:20:34.920716Z","shell.execute_reply":"2024-07-28T10:20:34.934006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"targets","metadata":{"execution":{"iopub.status.busy":"2024-07-28T04:38:36.515843Z","iopub.execute_input":"2024-07-28T04:38:36.516293Z","iopub.status.idle":"2024-07-28T04:38:36.524455Z","shell.execute_reply.started":"2024-07-28T04:38:36.516261Z","shell.execute_reply":"2024-07-28T04:38:36.523388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"1000/64","metadata":{"execution":{"iopub.status.busy":"2024-07-28T04:56:52.206349Z","iopub.execute_input":"2024-07-28T04:56:52.206750Z","iopub.status.idle":"2024-07-28T04:56:52.213415Z","shell.execute_reply.started":"2024-07-28T04:56:52.206718Z","shell.execute_reply":"2024-07-28T04:56:52.212266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_sample_count = torch.tensor([(targets == t).sum() for t in torch.unique(targets)])\nweight = 1. / class_sample_count.float()\nsamples_weight = torch.tensor([weight[int(t)] for t in targets])\n#samples_weight\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:20:39.682556Z","iopub.execute_input":"2024-07-28T10:20:39.682913Z","iopub.status.idle":"2024-07-28T10:20:39.696938Z","shell.execute_reply.started":"2024-07-28T10:20:39.682883Z","shell.execute_reply":"2024-07-28T10:20:39.696136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\nprint(sampler)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T10:20:56.999215Z","iopub.execute_input":"2024-07-28T10:20:57.000065Z","iopub.status.idle":"2024-07-28T10:20:57.004862Z","shell.execute_reply.started":"2024-07-28T10:20:57.000029Z","shell.execute_reply":"2024-07-28T10:20:57.004040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nprint(len(sampler))\nprint(\"dataset: \", len(dataset))\ntrain_loader = DataLoader(dataset, batch_size=64, sampler=sampler)\nprint(\"train_loader size: \", len(train_loader))\nfor batch_data, batch_target in train_loader:\n    print(batch_data, batch_target) ","metadata":{"execution":{"iopub.status.busy":"2024-07-28T04:58:57.804040Z","iopub.execute_input":"2024-07-28T04:58:57.804972Z","iopub.status.idle":"2024-07-28T04:58:57.919235Z","shell.execute_reply.started":"2024-07-28T04:58:57.804932Z","shell.execute_reply":"2024-07-28T04:58:57.917810Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}