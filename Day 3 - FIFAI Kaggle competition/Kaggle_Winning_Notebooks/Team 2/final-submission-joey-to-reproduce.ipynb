{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":83099,"databundleVersionId":9169362,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Key points of the best submission (Balancing - training steps (epochs, rounds) - TTA-capable model)\n\n1. There should be mulitple rounds for the proper Federated learning. - local epochs reduced (5->3) Global rounds : 20\n2. To mitigate the imbalance of labels, use Sampler for dataloader. This ensures the model is trained with balanced dataset (iid)\n3. Modification in the model (SimpleCNN), to allow the effect of Test-Time Adaptation later - Insert Batchnorm and replace relu with LeakyReRU\n4. Model was overfitted with batchnorms -> weight decay (0.01) in optimizer, this prevents the overfitting while keeping Batchnorms\n5. Reduce local epochs as mentioned in 1.\n6. Test-time adaptation - Tent approach allows the model to adapt the unseen dataset without any labels. No harm to apply it and see the results. To maximize TTA effect, increase batch size as much as I can, and insert batchnorm in the model","metadata":{"id":"TgeIcJO_6cx5"}},{"cell_type":"markdown","source":"# Setup","metadata":{"id":"YEg6aM9V39Kw"}},{"cell_type":"markdown","source":"The following libraries are required to run this notebook. If you are running this on Colab it should be all smooth sailing. If you are running it locally please make sure you have all of these installed (take a look at the readme file at the [repo](https://github.com/RISE-MICCAI/FIFAI-Summer-School-2024/tree/main/Day%203%20-%20FIFAI%20Kaggle%20competition) for instructions on how to do this with conda).","metadata":{"id":"AuVNaNZUMPaV"}},{"cell_type":"code","source":"import os\nimport random\nimport zipfile\nfrom collections import defaultdict\n\nfrom tqdm import tqdm\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset, DataLoader, Subset\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\nimport pandas as pd\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import label_binarize\nimport seaborn as sns","metadata":{"id":"WKanyuPFDjlt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Dataloading","metadata":{"id":"MxhO4N4MFF7q"}},{"cell_type":"markdown","source":"First thing we need to do is load in the data. We will be looking at the ChestMNIST dataset (a multi-label dataset from [MedMNIST](https://github.com/MedMNIST/MedMNIST)). This dataset looks at images of lungs that are healthy vs unhealthy for various diseases. We have taken a subset of this data and converted into a binary classification dataset. It consists of a training set with both images and labels and a test set with just images which you will use to make predictions and submit to Kaggle. Be sure to **download all** the files on the Kaggle site before proceeding with the next steps.\n\n\n\n","metadata":{"id":"jXWuhLGWxS_h"}},{"cell_type":"markdown","source":"### Using Colab (skip if running locally)\nThe below is for use in a Colab notebook. Make a new folder called 'FIFAI_Kaggle' in your own Google Drive account. Simply upload everything into this 'FIFAI_Kaggle' folder. **NB** compress the images folder into a .zip file before uploading it otherwise it will take very long to upload. So upload 'images.zip', 'train.csv', 'test.csv', 'train_dataset.pkl' and 'test_dataset.pkl'. After that you can run the blocks of code below. First let's create a link to this new folder you created for ease of use.","metadata":{"id":"I5sLkv9UinmV"}},{"cell_type":"code","source":"# mount drive and create symlink\nfrom google.colab import drive\ndrive.mount('/content/drive')\n# data_folder_name = 'FIFAI_Kaggle'\ndata_folder_name = \"/content/drive/My Drive/FIFAI_Kaggle\"\n# make sure to check if this is the correct directory for your Google account some people do not have a space in 'My Drive'\n# !ln -s \"/content/drive/My Drive/FIFAI_Kaggle\" \"/content/FIFAI_Kaggle\" # NB when changing this","metadata":{"id":"aeVtsaROccV2","outputId":"6c770126-ba46-449b-93eb-17c58c5802fc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unzip the images zip file. This should take about 2-3 mins. **Do not** run it again if you have already unzipped the folder. No need to run this block if you are running this locally.","metadata":{"id":"k97KYnBJngLz"}},{"cell_type":"code","source":"# only run this once to unzip your files\nimport zipfile\nimport os\n\ndef unzip_folder(folder_path, extract_to=None):\n    \"\"\"\n    Unzip the given folder.\n\n    Parameters:\n    folder_path (str): Path to the zipped folder.\n    extract_to (str): Directory to extract files to. Defaults to the same directory as the zipped folder.\n\n    Returns:\n    None\n    \"\"\"\n    if extract_to is None:\n        extract_to = os.path.dirname(folder_path)\n\n    with zipfile.ZipFile(folder_path, 'r') as zip_ref:\n        zip_ref.extractall(extract_to)\n    print(f'Extracted all files to {extract_to}')\n\n# Example usage:\ncwd = os.getcwd()\nunzip_folder(os.path.join(cwd,data_folder_name,'images.zip'), extract_to = os.path.join(cwd,data_folder_name))\n","metadata":{"id":"Sm6P_2x-l26R","outputId":"cdeb70cf-dfee-4e50-bb1e-bb2550c5644d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_folder_name = '/path_to_kaggle_files_on_local_dir' # uncomment and change this to the correct directory if running locally\n\n# if you are running this locally make sure you change these directories appropriately\ntrain_img_dir = os.path.join(data_folder_name, \"images\", \"train\")\ntest_img_dir = os.path.join(data_folder_name, \"images\", \"test\")\ntrain_csv_path = os.path.join(data_folder_name,\"train.csv\")","metadata":{"id":"hgs5RePXcpDG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we define appropriate classes and functions to help with dataloading when training our models.","metadata":{"id":"WaKKOs00n3Je"}},{"cell_type":"code","source":"# Create a PyTorch Dataset\nclass ChestMNISTDataset(Dataset):\n    \"\"\"\n    This class is used to turn our dataset into a PyTorch-supported dataset.\n\n    Parameters:\n    - images: The images array from the MedMNIST dataset.\n    - labels: The corresponding labels array for each image.\n    - transform: Optional transformation to be applied to the images.\n    \"\"\"\n    def __init__(self, images, labels, transform=None):\n        self.images = images\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        label = torch.tensor(self.labels[idx])\n        if self.transform:\n            image = self.transform(image)\n        return image, label\n\n\n  # build dataset function which takes in Dataset class, img_directory and labels to generate the appropriate Dataset\ndef build_train_dataset(img_dir, img2label, transform=None):\n    # run through img directory store images as np.array in images array. Simultaneously store labels in numpy array, using img2label. Then convert labels to tensor (or you can do this in the Dataset class).\n    print('---------- Building Training Dataset ----------')\n    # print('...')\n    images = []\n    labels = []\n    total_images = len(img2label)\n    step = total_images // 10  # 10% step\n\n    for i, img_fname in enumerate(img2label):\n        img_path = os.path.join(img_dir, img_fname)\n        img = np.array(Image.open(img_path))\n        label = img2label[img_fname]\n        images.append(img)\n        labels.append(label)\n\n        # Print progress\n        if (i + 1) % step == 0:\n            progress = (i + 1) // step * 10\n            print(f'{\"-\" * (progress // 10)} {progress}% complete')\n\n    print('---------- Finished Training Dataset ----------')\n    print()\n    return ChestMNISTDataset(images, labels, transform=transform)\n","metadata":{"id":"18GCMUH6wDY1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path) # get the csv containing the labels for the training data\nimg2label = {} # build a dictionary to help us associate each image with the appropriate label\nfor i in range(len(train_df['ID'])):\n  img2label[train_df['ID'][i]] = train_df['label'][i]","metadata":{"id":"a_4n9ziH22CN","outputId":"57ca5b95-5411-4600-ea0e-2787922b237d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set a seed DO NOT change this\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# preprocessing (transforms we will apply to our images)\ndata_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[.5], std=[.5])\n])\n\ntrain_dataset = build_train_dataset(train_img_dir, img2label, transform = data_transform)","metadata":{"id":"9CdcixWp70yE","outputId":"1cbfda55-7a59-4be1-de1e-867d94986abe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Federated Learning","metadata":{"id":"HxM9JIbZIs-0"}},{"cell_type":"markdown","source":"In this competition, we will be dealing with a specialised form of machine learning called *federated learning* which you should have heard a *little* bit about in the days prior. To do this we will simulate a federated learning setup right here in this notebook using some simple classes and functions. These were designed to be very easy to use so during the competition you will not need to change much in this setup. It works by having Client objects who will act as our nodes which we will train on locally, and a Server object which acts as our centralised server containing the global model. The aggregation strategy will be fixed here to **FedAvg** for simplicity.\n\nQuick reminder, the equation for **FedAvg** [1] is the following:\n\n $$w^{t+1} = \\dfrac{1}{n}\\sum_{k=1}^{K} w_{k}^{t+1}$$\n\nEach term represents:\n* $w^{t+1}$: Updated global model parameters after round $t$.\n* $n$: Total number of clients.\n* $w_{k}^{t+1}$: Updated model parameters from client $k$ after local training.\n\nWhen sending the model from the Server to the Client we will do a local average instead of overwriting the local model weights (with the global model) to preserve local structure in the local models at the respective Clients (take a look at 'send_with_local_ave' within the Server class).","metadata":{"id":"HWhMT0MoIFLM"}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\n\n\nclass Client:\n    \"\"\"\n    A class representing a client in a federated learning setup.\n\n    Attributes:\n    - device: The device on which the client's model and data will be loaded (CPU or CUDA).\n    - model: The initial model assigned to the client.\n    - data: The dataset assigned to the client.\n    - id: A unique identifier for the client.\n    \"\"\"\n    def __init__(self, initial_model, data, id):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.model = initial_model.to(self.device)\n        self.data = data\n        self.id = id\n\n    def get_model_params(self):\n        \"\"\"\n        Returns the state dictionary of the client's model.\n\n        Returns:\n        - state_dict: The state dictionary of the client's model.\n        \"\"\"\n        return self.model.state_dict()\n\n\nclass Server:\n    \"\"\"\n    A class representing a central server in a federated learning setup.\n\n    Attributes:\n    - device: The device on which the server's global model will be loaded (CPU or CUDA).\n    - global_model: The global model maintained by the server.\n    - send_funcs: A dictionary mapping boolean flags to the corresponding send functions.\n    - send: The send function to be used (with or without local averaging) mimicking sending of weights from server to client.\n    \"\"\"\n    def __init__(self, model, local_weight_ave=True):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.global_model = model.to(self.device)\n        self.send_funcs = {\n            True: self.send_with_local_ave,\n            False: self.send_without_local_ave\n        }\n        self.send = self.send_funcs[local_weight_ave]\n\n    def send_without_local_ave(self, client):\n        \"\"\"\n        Sends the global model to the client without local averaging.\n\n        Parameters:\n        - client: The client to which the global model will be sent.\n        \"\"\"\n        client.model.load_state_dict(self.global_model.state_dict())\n        print(f\"Sent model to Client {client.id}\")\n\n    def send_with_local_ave(self, client):\n        \"\"\"\n        Sends the global model to the client with local averaging.\n\n        Parameters:\n        - client: The client to which the averaged model will be sent.\n        \"\"\"\n        global_params = self.global_model.state_dict()\n        local_params = client.get_model_params()\n        averaged_params = {key: (global_params[key] + local_params[key]) / 2 for key in global_params.keys()}\n        client.model.load_state_dict(averaged_params)\n        print(f\"Sent model to Client {client.id}\")\n    # 3-1, increase batch size to maximize the effect of batchnorm - this is a preparation step for the future TTAs\n    def train(self, clients, gen_optimizer, loss_fn, num_epochs, batch_size=128, shuffle=True, lr=0.01):\n        \"\"\"\n        Trains the global model using data from multiple clients.\n\n        Parameters:\n        - clients: A list of Client objects.\n        - gen_optimizer: The optimizer generator function (basically a wrapper of a PyTorch optimiser object).\n        - loss_fn: The loss function to be used for training.\n        - num_epochs: The number of epochs for training.\n        - batch_size: The batch size for data loading (default is 8).\n        - shuffle: Whether to shuffle the data (default is True).\n        - lr: Learning rate for the optimizer (default is 0.01).\n        \"\"\"\n        # 1. The number of rounds is required for long training\n        for i in range(20):\n          client_weights = []\n          for client in clients:\n              self.send(client)\n          # 2. Balancing labels - non-iid -> iid using sampler\n              class_sample_counts=[sum(np.array(client.data.dataset.labels)[client.data.indices]==0),sum(np.array(client.data.dataset.labels)[client.data.indices]==1)]\n              weights = 1. / torch.tensor(class_sample_counts, dtype=torch.float)\n              samples_weights = weights[np.array(client.data.dataset.labels)[client.data.indices]]\n              sampler = torch.utils.data.WeightedRandomSampler(\n                  weights=samples_weights,\n                  num_samples=len(samples_weights),\n                  replacement=True)\n              data_loader = DataLoader(client.data, batch_size=batch_size, sampler=sampler)\n              optimizer = gen_optimizer(params=client.model.parameters(), lr=lr)\n              client.model.train()\n              print(f'Begin local training at Client {client.id}')\n              for epoch in range(num_epochs):\n                  losses = []\n                  for data, target in data_loader:\n                      data, target = data.to(self.device), target.to(self.device)\n                      optimizer.zero_grad()\n                      # print(f\"loaded data\")\n                      output = client.model(data)\n                      loss = loss_fn(output, target)\n                      loss.backward()\n                      optimizer.step()\n                      losses.append(loss.clone().detach())\n                  ave_loss = torch.mean(torch.tensor(losses))\n                  print(f'Training loss, running average over batches, at end of epoch {epoch} is {ave_loss}')\n              print(f'End of local training at Client {client.id}')\n              client_weights.append(client.get_model_params())\n              print('------------------------------------------------------------------')\n\n          avg_model_params = self.aggregate_weights(client_weights)\n          self.global_model.load_state_dict(avg_model_params)\n\n    def aggregate_weights(self, client_weights):\n        \"\"\"\n        Aggregates the weights from multiple clients using simple averaging.\n\n        Parameters:\n        - client_weights: A list of state dictionaries from clients.\n\n        Returns:\n        - avg_params: The averaged state dictionary.\n        \"\"\"\n        avg_params = {key: torch.zeros_like(param) for key, param in client_weights[0].items()}\n        for params in client_weights:\n            for key in params.keys():\n                avg_params[key]=avg_params[key].float()\n                avg_params[key] += params[key] / len(client_weights)\n        return avg_params\n\n    # need a nice test function here that incorporates all of your \"FAIR\" metrics\n    # def test(self, validation_dataset, batch_size=64, device=None):\n\n\ndef setup_federated_simulation_env(model, client_names, dataset, data_weights, local_weight_ave=True):\n    \"\"\"\n    Sets up the federated learning simulation environment.\n\n    Parameters:\n    - model: The initial global model.\n    - client_names: A list of names for the clients.\n    - dataset: The dataset to be split among the clients.\n    - data_weights: A list of weights specifying the proportion of data for each client.\n    - local_weight_ave: A boolean flag indicating whether to use local weight averaging (default is True).\n\n    Returns:\n    - central_server: The Server object.\n    - clients: A list of Client objects.\n    \"\"\"\n    print('---------- Setting up Federating Learning Environment ----------')\n    print('...')\n    central_server = Server(model, local_weight_ave=local_weight_ave)\n    if len(client_names) != len(data_weights):\n        raise ValueError(\"Number of clients does not match proposed data splitting given data weights.\")\n    subset_datasets = split_dataset(dataset, data_weights)\n    clients = [Client(model, subset_dataset, client_name) for client_name, subset_dataset in zip(client_names, subset_datasets)]\n    print('---------- Finished Setting up Federating Learning Environment ----------')\n    print()\n    return central_server, clients\n\n\n\n# helper function to split the datasets\nimport numpy as np\nfrom torch.utils.data import Subset\n\ndef split_dataset(dataset, data_weights):\n    \"\"\"\n    Splits a dataset into multiple subsets according to specified weights.\n\n    Parameters:\n    - dataset: The original dataset to be split.\n    - data_weights: A list of weights specifying the proportion of each subset.\n                    The sum of data_weights should be 1.\n\n    Returns:\n    - A list of dataset objects, each corresponding to a subset of the original dataset.\n    \"\"\"\n    if not np.isclose(np.sum(data_weights), 1):\n      raise ValueError('Invalid data_weights, do not sum to 1.')\n    total_samples = len(dataset)\n    indices = np.arange(total_samples)\n    np.random.shuffle(indices)\n\n    split_indices = []\n    current_idx = 0\n    for weight in data_weights:\n        split_size = int(np.floor(weight * total_samples))\n        split_indices.append(indices[current_idx:current_idx + split_size])\n        current_idx += split_size\n\n    # Ensure that all samples are included (add the remaining samples to the last subset)\n    if current_idx < total_samples:\n        split_indices[-1] = np.concatenate((split_indices[-1], indices[current_idx:]), axis=0)\n\n    # Create the subset datasets\n    subset_datasets = [Subset(dataset, indices) for indices in split_indices]\n\n    return subset_datasets","metadata":{"id":"tXEv9fbhFK1g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Federated setup is designed such that the only \"things\" to vary is the model, the optimiser and the loss function. When we eventually get to the training run it will be clear how to do this but before that let's define some basic models to use.","metadata":{"id":"A7lC0AiVAfc1"}},{"cell_type":"markdown","source":"Here we have included some very simple models that you can start off with.","metadata":{"id":"xE0OvVUXp9SD"}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\n# simple MLP\nclass SimpleMLP(nn.Module):\n    def __init__(self, input_dim=28*28*1, hidden_dim=128, output_dim=2):\n        super(SimpleMLP, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, output_dim)\n        self.lrelu = nn.LeakyReLU()\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten the input tensor\n        x = self.lrelu(self.fc1(x))\n        out = self.fc2(x)\n        return out\n\n# Logistic Regression\nclass LogisticRegressor(nn.Module):\n    def __init__(self, input_dim=28*28*1, output_dim=2):\n        super(LogisticRegressor, self).__init__()\n        self.fc = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        x = x.view(x.size(0), -1)  # Flatten the input tensor\n        out = self.fc(x)\n        return out\n\n\n# SimpleCNN architecture\nclass SimpleCNN(nn.Module):\n    \"\"\"\n    # 3. Insert Batchnorm layers and replace relu with leakyRELU\n    \"\"\"\n    def __init__(self, num_classes=2):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=28, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=28, out_channels=64, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(in_features=64*7*7, out_features=128)\n        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n        #self.relu = nn.ReLU()\n        self.lrelu = nn.LeakyReLU()\n        self.bnorm1 = nn.BatchNorm2d(28)\n        self.bnorm2 = nn.BatchNorm2d(64)\n\n    def forward(self, x):\n        x = self.pool(self.lrelu(self.bnorm1(self.conv1(x))))\n        x = self.pool(self.lrelu(self.bnorm2(self.conv2(x))))\n        x = x.view(-1, 64*7*7) # this flattens the tensor for FC layer\n        x = self.lrelu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n\n# DenseNet architecture\nclass DenseNet(nn.Module):\n    def __init__(self, num_classes=2):\n        super(DenseNet, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1, padding=1)\n        self.dense = models.densenet121()\n        self.fc1 = nn.Linear(in_features=1000, out_features=128)\n        self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.fc2(self.relu(self.fc1(self.dense(self.conv1(x)))))\n        return x\n\n\n\n","metadata":{"id":"YpG4hhwaBHwU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's initialize our Federated Learning simulation and perform a demo training run to see how it all works together. Pay attention particularly to the 'gen_optimizer' and the 'criterion' functions. In our simulation, we are training locally, and because some optimizers, like Adam, leverage features like gradient history, we need to initialize our optimizers at the Client. To achieve this, we generate the optimizer locally when we train, which is slightly different from simply feeding in a PyTorch optimizer.","metadata":{"id":"jhCHLdSxC_BC"}},{"cell_type":"markdown","source":"Instead of re-building the dataset everytime, we will just load in a pre-pickled Python object.","metadata":{"id":"S6LEpGZS7D52"}},{"cell_type":"code","source":"import pickle\n\n# load in our pickled dataset\nwith open(os.path.join(data_folder_name, \"train_dataset.pkl\"), 'rb') as f:\n    train_dataset = pickle.load(f)","metadata":{"id":"2HTAXNQG7IOT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# set a seed DO NOT change this\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\n\ntorch.cuda.manual_seed(42)\ntorch.cuda.manual_seed_all(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nmodel = SimpleCNN()\nglobal_model = model\n\n\nclient_names = ['Hospital 1', 'Hospital 2','Hospital 3', 'Hospital 4']\ndata_weights = [0.25, 0.25, 0.25, 0.25]\nserver, clients =  setup_federated_simulation_env(model = global_model, client_names = client_names, dataset = train_dataset, data_weights = data_weights, local_weight_ave = True)\n\n# Define optimizer and loss function\ndef gen_optimizer(params, lr):\n    # 4. Weight_decay can prevent overfitting which caused by more learnable parameters in batchnorm\n    return torch.optim.SGD(params=params, lr=lr, weight_decay=0.01)\n\ndef criterion(input, target):\n  loss_fn = torch.nn.CrossEntropyLoss()\n  return loss_fn(input, target)\n\n# Train using Federated Averaging\n# 5. Reduce local epochs\nserver.train(clients, gen_optimizer, loss_fn = criterion, num_epochs=3, lr = 0.001) # can quickly see the effect of a large learning rate","metadata":{"id":"8I8xT1ydAi5t","outputId":"00ac418f-fb9b-4491-cc4b-40dc117ad168"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 📊 Evaluation","metadata":{"id":"VZrkfDmXMKzb"}},{"cell_type":"markdown","source":"A focus of this competition is to learn about different metrics that are used in classification, to try mitigate bias or imbalances in data. Lets look at some more \"conventional\" metrics first.","metadata":{"id":"vc_ycq-_MO7x"}},{"cell_type":"markdown","source":"When evaluating the performance of a binary classification model, several metrics are commonly used. Below, we explain and provide the formulas for some of these key metrics: Accuracy, Confusion Matrix, Sensitivity, and Specificity.\n\n## 1. Accuracy\n\n**Accuracy** is the ratio of correctly predicted instances to the total instances. It is a measure of the overall effectiveness of the classifier.\n\n**Formula:**\n\n$$ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} $$\n\nWhere:\n- \\( TP \\) = True Positives\n- \\( TN \\) = True Negatives\n- \\( FP \\) = False Positives\n- \\( FN \\) = False Negatives\n\n## 2. Confusion Matrix\n\nA **Confusion Matrix** is a table used to describe the performance of a classification model on a set of test data for which the true values are known. It allows the visualization of the performance of an algorithm.\n\n\n\\begin{array}{c|c|c}\n\\text{Actual / Predicted} & \\text{Positive} & \\text{Negative} \\\\\n\\hline\n\\text{Positive} & TP & FN \\\\\n\\text{Negative} & FP & TN \\\\\n\\end{array}\n\n\n## 3. Sensitivity (Recall)\n\n**Sensitivity**, also known as **Recall**, measures the proportion of actual positives that are correctly identified by the model. It focuses on the ability of the classifier to find all the positive samples.\n\n**Formula:**\n\n\n$$ \\text{Sensitivity} = \\frac{TP}{TP + FN} $$\n\n\n## 4. Specificity\n\n**Specificity** measures the proportion of actual negatives that are correctly identified by the model. It focuses on the ability of the classifier to find all the negative samples.\n\n**Formula:**\n\n\n$$ \\text{Specificity} = \\frac{TN}{TN + FP} $$\n\n\n## Summary\n\nThese metrics help in understanding the performance of a binary classification model from different perspectives. While accuracy gives a general measure of performance, the confusion matrix provides a detailed breakdown, and sensitivity and specificity offer insights into how well the model performs on the positive and negative classes, respectively.\n","metadata":{"id":"fZ0pf2_pOS4c"}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nfrom collections import defaultdict\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import label_binarize\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import confusion_matrix\n\ndef get_accuracy(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes overall accuracy for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 128).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - overall_accuracy: The overall accuracy over the entire dataset.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n\n    # Compute overall accuracy\n    overall_accuracy = (all_outputs.argmax(dim=1) == all_targets).float().mean().item()\n\n    return overall_accuracy\n\ndef get_confusion_matrix(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes and displays the confusion matrix for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 64).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - cm: Confusion matrix\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    binarized_targets = (all_targets == 1).to(torch.int)\n    binarized_preds = (all_outputs.argmax(dim=1) == 1).to(torch.int)\n    cm = confusion_matrix(binarized_targets, binarized_preds)\n\n    # Plot confusion matrix\n    plt.figure(figsize=(6, 5))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.title(f'Confusion Matrix for Predicting Disease')\n    plt.show()\n\n    return cm\n\ndef get_sensitivity(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes the sensitivity metric for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 1228).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - sensitivity: The sensitivity metric.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    binarized_targets = (all_targets == 1).to(torch.int)\n    binarized_preds = (all_outputs.argmax(dim=1) == 1).to(torch.int)\n    cm = confusion_matrix(binarized_targets, binarized_preds)\n\n    # Extract True Positives, False Positives, True Negatives, False Negatives\n    TP = cm[1, 1]\n    FP = cm[0, 1]\n    TN = cm[0, 0]\n    FN = cm[1, 0]\n\n    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n    return sensitivity\n\ndef get_specificity(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes the specificity metric for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 1228).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - specificity: The specificity metric.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n    binarized_targets = (all_targets == 1).to(torch.int)\n    binarized_preds = (all_outputs.argmax(dim=1) == 1).to(torch.int)\n    cm = confusion_matrix(binarized_targets, binarized_preds)\n\n    # Extract True Positives, False Positives, True Negatives, False Negatives\n    TP = cm[1, 1]\n    FP = cm[0, 1]\n    TN = cm[0, 0]\n    FN = cm[1, 0]\n    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n\n    return specificity\n","metadata":{"id":"qXymqADVM4W2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets test this on one of our clients data.","metadata":{"id":"S-Q7wh2cPq7V"}},{"cell_type":"code","source":"# lets get the accuracy\nget_accuracy(clients[0].data, clients[0].model)","metadata":{"id":"xtMRvqQfPuH4","outputId":"359eceec-bd06-4a97-faae-2de22d2f76fe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We have only trained our model for 5 epochs but we are already getting 79\\% accuracy. It appears that this is a fairly easy problem for us but before we simply run a few more epochs and close shop let's take a closer look by observing the 'confusion matrix'.","metadata":{"id":"DxNJ9NY1PySp"}},{"cell_type":"code","source":"get_confusion_matrix(clients[0].data, clients[0].model)","metadata":{"id":"DsKIWneZQQ-l","outputId":"d357ec9d-efd1-4ab1-9ee4-34bfb296da63"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This paints a clearer picture, even though our accuracy is high we can see that this is just a consequence of an imbalance in the data. You can see we do not predict a single diseased example of a lung correctly. The model is simply guessing \"healthy\" for every single sample (corresponding to a negative prediction for us). This is a common problem with metrics such as accuracy for data which is highly imbalanced. Let's take a look at the specificity and sensitivity metrics next.","metadata":{"id":"hhmiBsqtQUyp"}},{"cell_type":"code","source":"specificity = get_specificity(clients[0].data, clients[0].model)\nsensitivity = get_sensitivity(clients[0].data, clients[0].model)\n\nprint(f'The specificity for {clients[0].id} is {specificity}')\nprint(f'The sensitivity for {clients[0].id} is {sensitivity}')","metadata":{"id":"bw01qOmIRPSS","outputId":"3bbda95a-2d0a-4777-ff95-d39294e44a62"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If you look up at the definitions this makes sense. Specificity is how many negative samples of the total number of negative samples we capture which is everything because our model simply predicts negative for everythig. Sensitivity takes a look at how many positive samples of the total positive samples we are able to capture. Idealy we would like some metrics that are able to account for these imbalances in a single metric.","metadata":{"id":"few6RuX2Rtak"}},{"cell_type":"markdown","source":"Let's introduce some new measures that are more robust and can better account for imbalances in data. These metrics and more recommendations can be found in this detailed Nature article [2].","metadata":{"id":"S2EcmimjT5Yf"}},{"cell_type":"markdown","source":"## 5. Balanced Accuracy\n\n**Balanced Accuracy** is the average of sensitivity and specificity. It is useful when the classes are imbalanced.\n\n**Formula:**\n\n\n$$ \\text{Balanced Accuracy} = \\frac{\\text{Sensitivity} + \\text{Specificity}}{2} $$\n\n## 6. Area Under the Receiver Operating Characteristic Curve (AUROC)\n\n**AUROC** is a performance measurement for classification problems at various threshold settings. ROC is a probability curve, and AUROC represents the degree or measure of separability. It tells how much the model is capable of distinguishing between classes.\n\n### Receiver Operating Characteristic (ROC) Curve\n\nThe **ROC Curve** is a graphical representation of the true positive rate (sensitivity) against the false positive rate (1-specificity) at various threshold settings (a threshold being a probablility value above which we consider a sample to belong to a certain class). It shows the trade-off between sensitivity and specificity.\n\n- **True Positive Rate (TPR)**: Same as Sensitivity\n- **False Positive Rate (FPR)**: $$ \\text{FPR} = \\frac{FP}{FP + TN} $$\n\nEach point on the curve corresponds to a FPR vs TPR for given probablity threshold [3].\n\n![ROCCurve.svg](data:image/svg+xml;base64,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)\n\n\n\n\n**AUROC** is the area under the ROC curve. The higher the AUROC, the better the model is at distinguishing between the positive and negative classes with perfect value of AUROC is 1. I know it can be quite tricky to grasp this concept if it is the first time you are seeing it. I would recommend reading this [blog](https://mlu-explain.github.io/roc-auc/) [4] which gives a pretty good explanation. It will also be more evident when looking at some concrete examples.\n\n\n## Summary 2\n\n Balanced accuracy and AUROC are metrics that are better adapted to dealing with imbalanced data. Balanced accuracy directly tackles the imbalance by averaging specificity and sensisitivity whilst AUROC assesses the model's overall ability to distinguish between classes i.e. how well it \"ranks\" the probabilities.\n","metadata":{"id":"RW5B1p8QTCRC"}},{"cell_type":"markdown","source":"Let's take a look at an example","metadata":{"id":"oQgR4YEdWmvt"}},{"cell_type":"code","source":"def get_balanced_accuracy(validation_dataset, model, batch_size=128, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')):\n    \"\"\"\n    Computes the balanced accuracy for the given model on the validation dataset.\n\n    Parameters:\n    - validation_dataset: The MedMNIST validation dataset.\n    - model: The PyTorch model to be evaluated.\n    - batch_size: Batch size for DataLoader (default is 1228).\n    - device: The device to run the evaluation on (default is CUDA if available).\n\n    Returns:\n    - balanced_accuracy: The average balanced accuracy over all classes.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    model.to(device)\n\n    # Create DataLoader for validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    all_targets = []\n    all_outputs = []\n\n    with torch.no_grad():\n        for inputs, targets in val_loader:\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            all_outputs.append(outputs.cpu())\n            all_targets.append(targets.cpu())\n\n    all_outputs = torch.cat(all_outputs)\n    all_targets = torch.cat(all_targets)\n\n    binarized_targets = (all_targets == 1).to(torch.int)\n    binarized_preds = (all_outputs.argmax(dim=1) == 1).to(torch.int)\n    cm = confusion_matrix(binarized_targets, binarized_preds)\n\n    # Extract True Positives, False Positives, True Negatives, False Negatives\n    TP = cm[1, 1]\n    FP = cm[0, 1]\n    TN = cm[0, 0]\n    FN = cm[1, 0]\n\n    sensitivity = TP / (TP + FN) if (TP + FN) != 0 else 0\n    specificity = TN / (TN + FP) if (TN + FP) != 0 else 0\n    balanced_acc = (sensitivity + specificity) / 2\n\n\n    # balanced_accuracy = sum(class_balanced_accuracies) / num_classes\n    return balanced_acc\n\n# NB change the doc string here because you are no longer doing multi-class\ndef get_auroc_plot_roc(validation_dataset, model, batch_size=128, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n    \"\"\"\n    Calculate the AUROC and plot the ROC curve for a binary classification problem.\n\n    Parameters:\n    validation_dataset (Dataset): The validation dataset.\n    model (nn.Module): The trained model.\n    batch_size (int): The batch size for data loading.\n    device (str): The device to run the model on ('cpu' or 'cuda').\n\n    Returns:\n    roc_auc: AUROC value.\n    \"\"\"\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Create a DataLoader for the validation dataset\n    val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n\n    # Lists to store true labels and predicted probabilities\n    all_labels = []\n    all_probs = []\n\n    # Disable gradient calculation for validation\n    with torch.no_grad():\n        for batch in val_loader:\n            # Move batch to the specified device\n            inputs, labels = batch\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Get model outputs (logits)\n            logits = model(inputs)\n\n            # Apply softmax to get probabilities\n            probabilities = F.softmax(logits, dim=1)\n\n            # Store labels and probabilities\n            all_labels.append(labels.cpu().numpy())\n            all_probs.append(probabilities.cpu().numpy())\n\n    # Concatenate all batches\n    all_labels = np.concatenate(all_labels)\n    all_probs = np.concatenate(all_probs)\n    all_probs = all_probs[:,1] # you need to only look at a single class for this metric\n\n\n    # Calculate ROC curve and AUROC\n\n    plt.figure(figsize=(12, 8))\n    fpr, tpr, _ = roc_curve(all_labels, all_probs)\n    roc_auc = auc(fpr, tpr)\n\n    plt.plot(fpr, tpr, lw=2, label=f' ROC model predictor (area = {roc_auc:.2f})')\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=2, label = 'ROC random predictor (area = 0.5)')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curves')\n    plt.legend(loc='lower right')\n    plt.grid()\n    plt.show()\n    return roc_auc","metadata":{"id":"pk3zjxz-RC7H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_balanced_accuracy(clients[0].data, clients[0].model)","metadata":{"id":"0OgrI1-qOMrH","outputId":"9243809a-0218-4093-c6bd-b4642e9b29fa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This is a much more reasonable metric, which reflects a \"truer\" performance of our model for our purposes. It is very easy to intuitivley see how we get this value. We get 100\\% of the negative label healthy samples but 0\\% of the positive label unhealthy samples so we are left with an average performance of 50\\%.","metadata":{"id":"Ikghx_8GXKvD"}},{"cell_type":"markdown","source":"Now let's take a look at the ROC curve and the Area under this curve.","metadata":{"id":"jGHosabyXN49"}},{"cell_type":"code","source":"get_auroc_plot_roc(clients[0].data, clients[0].model)","metadata":{"id":"iSrWaUc8XHnH","outputId":"856b109f-52d4-4699-c1dd-942a460a06af"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This also appears to give us a more \"sane\" performance metric as 0.60 is considerably less that the previous 0.79 we were getting with just accuracy.  ","metadata":{"id":"3omytAYGXVTh"}},{"cell_type":"markdown","source":"Feel free to use these functions to help you assess the performance of your model as you make submissions to help guide you along your fine-tuning process.","metadata":{"id":"oYL8B13fYHMR"}},{"cell_type":"markdown","source":"# Kaggle Submission","metadata":{"id":"_m543wxXZAub"}},{"cell_type":"markdown","source":"Finally we need to create a submission for our Kaggle competition which will be a csv file containing the predictions of our test images. We first define some classes and functions to help with this.","metadata":{"id":"3Yuea1a7wu4T"}},{"cell_type":"code","source":"# from inspect import isgetsetdescriptor\n# Create a PyTorch Dataset\nclass ChestMNISTDatasetTest(Dataset):\n    \"\"\"\n    This class is used to turn our dataset into a PyTorch-supported dataset.\n\n    Parameters:\n    - images: The images array from the MedMNIST dataset.\n    - labels: The corresponding labels array for each image.\n    - transform: Optional transformation to be applied to the images.\n    \"\"\"\n    def __init__(self, images, ids, transform=None):\n        self.images = images\n        self.ids = ids\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image = self.images[idx]\n        id = self.ids[idx]\n        if self.transform:\n            image = self.transform(image)\n        return image, id\n\ndef build_test_dataset(img_dir, transform=None):\n    # run through img directory store images as np.array in images array. Simultaneously store labels in numpy array, using img2label. Then convert labels to tensor (or you can do this in the Dataset class).\n    print('---------- Building Test Dataset ----------')\n    images = []\n    ids = []\n    img_fnames = os.listdir(img_dir)\n    total_images = len(img_fnames)\n    step = total_images // 10  # 10% step\n\n    for i, img_fname in enumerate(img_fnames):\n        img_path = os.path.join(img_dir, img_fname)\n        img = np.array(Image.open(img_path))\n        id = img_fname\n        images.append(img)\n        ids.append(id)\n\n        # Print progress\n        if (i + 1) % step == 0:\n            progress = (i + 1) // step * 10\n            print(f'{\"-\" * (progress // 10)} {progress}% complete ')\n\n    print('---------- Finished Building Test Dataset ----------')\n    print()\n    return ChestMNISTDatasetTest(images, ids, transform=transform)\n\nimport torch\nfrom torch.utils.data import DataLoader\nimport pandas as pd\n\n\ndef gen_test_df(global_model, test_dataset, device, batch_size=32):\n    # Set the model to evaluation mode\n    #global_model.eval()\n    global_model.train()\n\n    # Create a DataLoader for the test dataset\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Lists to store ids and predicted labels\n    all_ids = []\n    all_outputs = []\n\n    total_batches = len(test_loader)\n    step = total_batches // 10  # 10% step\n\n    # Disable gradient calculation for evaluation\n    print('---------- Inference on Test Dataset ----------')\n    with torch.no_grad():\n        for i, batch in enumerate(test_loader):\n            # Move batch to the specified device\n            inputs, input_ids = batch\n            inputs = inputs.to(device)\n            # 6-1 adatation steps - 500\n            for ad in range(500):\n              logits = global_model(inputs)\n            # Get model outputs (logits)\n            logits = global_model(inputs)\n\n            # Get the predicted labels\n            predicted_labels = logits.argmax(dim=1).cpu().numpy()\n\n            # Append ids and outputs to lists\n            all_ids.extend(list(input_ids))\n            all_outputs.extend(predicted_labels)\n\n            # Print progress\n            if (i + 1) % step == 0:\n                progress = (i + 1) // step * 10\n                print(f'{\"-\" * (progress // 10)} {progress}% complete')\n    print('---------- Completed Inference on Test Dataset ----------')\n    print()\n    # Create a DataFrame with the ids and predicted labels\n    results_df = pd.DataFrame({\n        'ID': all_ids,\n        'label': all_outputs\n    })\n    results_df_sorted = results_df.sort_values(by=['ID'], ascending=True)\n    return results_df_sorted\n\n","metadata":{"id":"gKGCscqFMShm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we use the above to generate the csv which we will use to make our submission. We first load in our test dataset.","metadata":{"id":"rq9l5AFvYyYo"}},{"cell_type":"code","source":"# preprocessing\ndata_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[.5], std=[.5])\n])\n\ntest_dataset = build_test_dataset(test_img_dir, transform = data_transform)","metadata":{"id":"bbBQ4rzAN0J9","outputId":"db85efbc-19c9-43cc-dffe-bf8747a9bfe2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As before, for ease of use, we have the test_dataset pre-pickled so you can just load it in.","metadata":{"id":"U40NxMgWAgMV"}},{"cell_type":"code","source":"import pickle\n\n# load in pickled test dataset\nwith open(os.path.join(data_folder_name, \"test_dataset.pkl\"), 'rb') as f:\n    test_dataset = pickle.load(f)","metadata":{"id":"xd9-9CduOKZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Test-Time Adaptation code (TENT)\n\nfrom copy import deepcopy\nimport torch\nimport torch.nn as nn\nimport torch.jit\n\n\nclass Tent(nn.Module):\n    \"\"\"Tent adapts a model by entropy minimization during testing.\n\n    Once tented, a model adapts itself by updating on every forward.\n    \"\"\"\n    def __init__(self, model, optimizer, steps=1, episodic=False):\n        super().__init__()\n        self.model = model\n        self.optimizer = optimizer\n        self.steps = steps\n        assert steps > 0, \"tent requires >= 1 step(s) to forward and update\"\n        self.episodic = episodic\n\n        # note: if the model is never reset, like for continual adaptation,\n        # then skipping the state copy would save memory\n        self.model_state, self.optimizer_state = \\\n            copy_model_and_optimizer(self.model, self.optimizer)\n\n    def forward(self, x):\n        if self.episodic:\n            self.reset()\n\n        for _ in range(self.steps):\n            outputs = forward_and_adapt(x, self.model, self.optimizer)\n\n        return outputs\n\n    def reset(self):\n        if self.model_state is None or self.optimizer_state is None:\n            raise Exception(\"cannot reset without saved model/optimizer state\")\n        load_model_and_optimizer(self.model, self.optimizer,\n                                 self.model_state, self.optimizer_state)\n\n\n@torch.jit.script\ndef softmax_entropy(x: torch.Tensor) -> torch.Tensor:\n    \"\"\"Entropy of softmax distribution from logits.\"\"\"\n    return -(x.softmax(1) * x.log_softmax(1)).sum(1)\n\n\n@torch.enable_grad()  # ensure grads in possible no grad context for testing\ndef forward_and_adapt(x, model, optimizer):\n    \"\"\"Forward and adapt model on batch of data.\n\n    Measure entropy of the model prediction, take gradients, and update params.\n    \"\"\"\n    # forward\n    outputs = model(x)\n    # adapt\n    loss = softmax_entropy(outputs).mean(0)\n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    return outputs\n\n\ndef collect_params(model):\n    \"\"\"Collect the affine scale + shift parameters from batch norms.\n\n    Walk the model's modules and collect all batch normalization parameters.\n    Return the parameters and their names.\n\n    Note: other choices of parameterization are possible!\n    \"\"\"\n    params = []\n    names = []\n    for nm, m in model.named_modules():\n        if isinstance(m, nn.BatchNorm2d):\n            for np, p in m.named_parameters():\n                if np in ['weight', 'bias']:  # weight is scale, bias is shift\n                    params.append(p)\n                    names.append(f\"{nm}.{np}\")\n    return params, names\n\n\ndef copy_model_and_optimizer(model, optimizer):\n    \"\"\"Copy the model and optimizer states for resetting after adaptation.\"\"\"\n    model_state = deepcopy(model.state_dict())\n    optimizer_state = deepcopy(optimizer.state_dict())\n    return model_state, optimizer_state\n\n\ndef load_model_and_optimizer(model, optimizer, model_state, optimizer_state):\n    \"\"\"Restore the model and optimizer states from copies.\"\"\"\n    model.load_state_dict(model_state, strict=True)\n    optimizer.load_state_dict(optimizer_state)\n\n\ndef configure_model(model):\n    \"\"\"Configure model for use with tent.\"\"\"\n    # train mode, because tent optimizes the model to minimize entropy\n    model.train()\n    # disable grad, to (re-)enable only what tent updates\n    model.requires_grad_(False)\n    # configure norm for tent updates: enable grad + force batch statisics\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.requires_grad_(True)\n            # force use of batch stats in train and eval modes\n            m.track_running_stats = False\n            m.running_mean = None\n            m.running_var = None\n    return model\n\n\ndef check_model(model):\n    \"\"\"Check model for compatability with tent.\"\"\"\n    is_training = model.training\n    assert is_training, \"tent needs train mode: call model.train()\"\n    param_grads = [p.requires_grad for p in model.parameters()]\n    has_any_params = any(param_grads)\n    has_all_params = all(param_grads)\n    assert has_any_params, \"tent needs params to update: \" \\\n                           \"check which require grad\"\n    assert not has_all_params, \"tent should not update all params: \" \\\n                               \"check which require grad\"\n    has_bn = any([isinstance(m, nn.BatchNorm2d) for m in model.modules()])\n    assert has_bn, \"tent needs normalization for its optimization\"","metadata":{"id":"zI_tRKrCOQax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = configure_model(model)\nparams, param_names = collect_params(model)\noptimizer = torch.optim.SGD(params, lr=1e-3)\ntented_model = Tent(model, optimizer)","metadata":{"id":"PU4UrxnNOTh9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Now we use our test_dataset object and our global model to generate our .csv submission file. **Note** during the competition when you are making submission you can just keep re-running the below block there is nothing you need to change.","metadata":{"id":"dIRLwj4jAUCe"}},{"cell_type":"code","source":"global_model = server.global_model\nglobal_model = configure_model(global_model)\nparams, param_names = collect_params(global_model)\noptimizer = torch.optim.SGD(params, lr=1e-3)\nglobal_model = Tent(global_model, optimizer)\ndevice = server.device\n# 6-2: increase batch size for the TTA performance\nresults_df = gen_test_df(global_model, test_dataset, device, batch_size=128)\n\nresults_df.to_csv( os.path.join( data_folder_name, \"sample_submission.csv\"), index=False) # save csv the chosen directory.","metadata":{"id":"W8smq30MSAEh","outputId":"0b6078d9-7eaf-4a96-9d8f-0a639c1bb99e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And that's a wrap! You should have all the tools you need to start playing around with your own models and begin making your own submissions. Good luck!","metadata":{"id":"HtUWeVNh2Xfb"}},{"cell_type":"markdown","source":"# References\n[1]   **(FedAvg)** McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017, April). Communication-efficient learning of deep networks from decentralized data. In Artificial intelligence and statistics (pp. 1273-1282). PMLR.\n\n[2] Maier-Hein, L., Reinke, A., Godau, P. et al. Metrics reloaded: recommendations for image analysis validation. Nat Methods 21, 195–212 (2024). https://doi.org/10.1038/s41592-023-02151-z\n\n[3] Google. “Classification: ROC Curve and AUC  |  Machine Learning Crash Course.” Google Developers, 2019, developers.google.com/machine-learning/crash-course/classification/roc-and-auc.\n\n[4] Wilber, Jared. “ROC and AUC.” MLU-Explain, mlu-explain.github.io/roc-auc/.\n\n\n","metadata":{"id":"SXdF6A2wJxEp"}}]}